{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Advances in Financial Machine Learning\n",
    "## CHAPTER 2 - Financial Data Structures\n",
    "### 2.4.2 PCA Weights\n",
    "#### SNIPPET 2.1 PCA WEIGHTS FROM A RISK DISTRIBUTION R\n",
    "Snippet 2.1 implements this method, where the user-defined risk distribution\n",
    "R is passed through argument riskDist (optional None). If riskDist is None,\n",
    "the code will assume all risk must be allocated to the principal component with\n",
    "smallest eigenvalue, and the weights will be the last eigenvector re-scaled to match\n",
    "ùúé (riskTarget)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pcaWeights(cov,riskDist=None,riskTarget=1.):\n",
    "    # Following the riskAlloc distribution, match riskTarget\n",
    "    eVal,eVec=np.linalg.eigh(cov) # must be Hermitian\n",
    "    indices=eVal.argsort()[::-1] # arguments for sorting eVal desc\n",
    "    eVal,eVec=eVal[indices],eVec[:,indices]\n",
    "    if riskDist is None:\n",
    "        riskDist=np.zeros(cov.shape[0])\n",
    "        riskDist[-1]=1.\n",
    "    loads=riskTarget*(riskDist/eVal)**.5\n",
    "    wghts=np.dot(eVec,np.reshape(loads,(-1,1)))\n",
    "    #ctr=(loads/riskTarget)**2*eVal # verify riskDist\n",
    "    return wghts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SNIPPET 2.2 FORM A GAPS SERIES, DETRACT IT FROM PRICES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getRolledSeries(pathIn,key):\n",
    "    series=pd.read_hdf(pathIn,key='bars/ES_10k')\n",
    "    series['Time']=pd.to_datetime(series['Time'],format='%Y%m%d%H%M%S%f')\n",
    "    series=series.set_index('Time')\n",
    "    gaps=rollGaps(series)\n",
    "    for fld in ['Close','VWAP']:series[fld]-=gaps\n",
    "    return series\n",
    "#‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî\n",
    "def rollGaps(series,dictio={'Instrument':'FUT_CUR_GEN_TICKER','Open':'PX_OPEN', \\\n",
    "    'Close':'PX_LAST'},matchEnd=True):\n",
    "    # Compute gaps at each roll, between previous close and next open\n",
    "    rollDates=series[dictio['Instrument']].drop_duplicates(keep='first').index\n",
    "    gaps=series[dictio['Close']]*0\n",
    "    iloc=list(series.index)\n",
    "    iloc=[iloc.index(i)-1 for i in rollDates] # index of days prior to roll\n",
    "    gaps.loc[rollDates[1:]]=series[dictio['Open']].loc[rollDates[1:]]- \\\n",
    "    series[dictio['Close']].iloc[iloc[1:]].values\n",
    "    gaps=gaps.cumsum()\n",
    "    if matchEnd:gaps-=gaps.iloc[-1] # roll backward\n",
    "    return gaps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SNIPPET 2.3 NON-NEGATIVE ROLLED PRICE SERIES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'filePath' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-6-87981abf82ef>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mraw\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilePath\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mindex_col\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mparse_dates\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[0mgaps\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mrollGaps\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mraw\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mdictio\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m{\u001b[0m\u001b[1;34m'Instrument'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;34m'Symbol'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'Open'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;34m'Open'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'Close'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;34m'Close'\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mrolled\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mraw\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdeep\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'filePath' is not defined"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "raw=pd.read_csv(filePath,index_col=0,parse_dates=True)\n",
    "gaps=rollGaps(raw,dictio={'Instrument':'Symbol','Open':'Open','Close':'Close'})\n",
    "rolled=raw.copy(deep=True)\n",
    "for fld in ['Open','Close']:rolled[fld]-=gaps\n",
    "rolled['Returns']=rolled['Close'].diff()/raw['Close'].shift(1)\n",
    "rolled['rPrices']=(1+rolled['Returns']).cumprod()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SNIPPET 2.4 THE SYMMETRIC CUSUM FILTER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getTEvents(gRaw,h):\n",
    "    tEvents,sPos,sNeg=[],0,0\n",
    "    diff=gRaw.diff()\n",
    "    for i in diff.index[1:]:\n",
    "        sPos,sNeg=max(0,sPos+diff.loc[i]),min(0,sNeg+diff.loc[i])\n",
    "        if sNeg<-h:\n",
    "            sNeg=0;tEvents.append(i)\n",
    "        elif sPos>h:\n",
    "            sPos=0;tEvents.append(i)\n",
    "    return pd.DatetimeIndex(tEvents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CHAPTER 3 - Labeling\n",
    "#### SNIPPET 3.1 DAILY VOLATILITY ESTIMATES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid character in identifier (<ipython-input-8-1b5ff353187c>, line 5)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-8-1b5ff353187c>\"\u001b[1;36m, line \u001b[1;32m5\u001b[0m\n\u001b[1;33m    df0=pd.Series(close.index[df0‚Äì1], index=close.index[close.shape[0]-df0.shape[0]:])\u001b[0m\n\u001b[1;37m                                  ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid character in identifier\n"
     ]
    }
   ],
   "source": [
    "def getDailyVol(close,span0=100):\n",
    "    # daily vol, reindexed to close\n",
    "    df0=close.index.searchsorted(close.index-pd.Timedelta(days=1))\n",
    "    df0=df0[df0>0]\n",
    "    df0=pd.Series(close.index[df0‚Äì1], index=close.index[close.shape[0]-df0.shape[0]:])\n",
    "    df0=close.loc[df0.index]/close.loc[df0.values].values-1 # daily returns\n",
    "    df0=df0.ewm(span=span0).std()\n",
    "    return df0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SNIPPET 3.2 TRIPLE-BARRIER LABELING METHOD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def applyPtSlOnT1(close,events,ptSl,molecule):\n",
    "    # apply stop loss/profit taking, if it takes place before t1 (end of event)\n",
    "    events_=events.loc[molecule]\n",
    "    out=events_[['t1']].copy(deep=True)\n",
    "    if ptSl[0]>0:pt=ptSl[0]*events_['trgt']\n",
    "    else:pt=pd.Series(index=events.index) # NaNs\n",
    "    if ptSl[1]>0:sl=-ptSl[1]*events_['trgt']\n",
    "    else:sl=pd.Series(index=events.index) # NaNs\n",
    "    for loc,t1 in events_['t1'].fillna(close.index[-1]).iteritems():\n",
    "        df0=close[loc:t1] # path prices\n",
    "        df0=(df0/close[loc]-1)*events_.at[loc,'side'] # path returns\n",
    "        out.loc[loc,'sl']=df0[df0<sl[loc]].index.min() # earliest stop loss.\n",
    "        out.loc[loc,'pt']=df0[df0>pt[loc]].index.min() # earliest profit taking.\n",
    "    return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SNIPPET 3.3 GETTING THE TIME OF FIRST TOUCH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getEvents(close,tEvents,ptSl,trgt,minRet,numThreads,t1=False):\n",
    "    #1) get target\n",
    "    trgt=trgt.loc[tEvents]\n",
    "    trgt=trgt[trgt>minRet] # minRet\n",
    "    #2) get t1 (max holding period)\n",
    "    if t1 is False:t1=pd.Series(pd.NaT,index=tEvents)\n",
    "    #3) form events object, apply stop loss on t1\n",
    "    side_=pd.Series(1.,index=trgt.index)\n",
    "    events=pd.concat({'t1':t1,'trgt':trgt,'side':side_}, \\\n",
    "        axis=1).dropna(subset=['trgt'])\n",
    "    df0=mpPandasObj(func=applyPtSlOnT1,pdObj=('molecule',events.index), \\\n",
    "        numThreads=numThreads,close=close,events=events,ptSl=[ptSl,ptSl])\n",
    "    events['t1']=df0.dropna(how='all').min(axis=1) # pd.min ignores nan\n",
    "    events=events.drop('side',axis=1)\n",
    "    return events"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SNIPPET 3.4 ADDING A VERTICAL BARRIER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'close' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-11-dd254505aa3c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mt1\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mclose\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msearchsorted\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtEvents\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTimedelta\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdays\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnumDays\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mt1\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mt1\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mt1\u001b[0m\u001b[1;33m<\u001b[0m\u001b[0mclose\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mt1\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mSeries\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mclose\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mt1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtEvents\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mt1\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m# NaNs at end\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'close' is not defined"
     ]
    }
   ],
   "source": [
    "t1=close.index.searchsorted(tEvents+pd.Timedelta(days=numDays))\n",
    "t1=t1[t1<close.shape[0]]\n",
    "t1=pd.Series(close.index[t1],index=tEvents[:t1.shape[0]]) # NaNs at end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SNIPPET 3.5 LABELING FOR SIDE AND SIZE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getBins(events,close):\n",
    "    #1) prices aligned with events\n",
    "    events_=events.dropna(subset=['t1'])\n",
    "    px=events_.index.union(events_['t1'].values).drop_duplicates()\n",
    "    px=close.reindex(px,method='bfill')\n",
    "    #2) create out object\n",
    "    out=pd.DataFrame(index=events_.index)\n",
    "    out['ret']=px.loc[events_['t1'].values].values/px.loc[events_.index]-1\n",
    "    out['bin']=np.sign(out['ret'])\n",
    "    return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SNIPPET 3.6 EXPANDING getEvents TO INCORPORATE META-LABELING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getEvents(close,tEvents,ptSl,trgt,minRet,numThreads,t1=False,side=None):\n",
    "    #1) get target\n",
    "    trgt=trgt.loc[tEvents]\n",
    "    trgt=trgt[trgt>minRet] # minRet\n",
    "    #2) get t1 (max holding period)\n",
    "    if t1 is False:t1=pd.Series(pd.NaT,index=tEvents)\n",
    "    #3) form events object, apply stop loss on t1\n",
    "    if side is None:side_,ptSl_=pd.Series(1.,index=trgt.index),[ptSl[0],ptSl[0]]\n",
    "    else:side_,ptSl_=side.loc[trgt.index],ptSl[:2]\n",
    "    events=pd.concat({'t1':t1,'trgt':trgt,'side':side_}, \\\n",
    "        axis=1).dropna(subset=['trgt'])\n",
    "    df0=mpPandasObj(func=applyPtSlOnT1,pdObj=('molecule',events.index), \\\n",
    "        numThreads=numThreads,close=inst['Close'],events=events,ptSl=ptSl_)\n",
    "    events['t1']=df0.dropna(how='all').min(axis=1) # pd.min ignores nan\n",
    "    if side is None:events=events.drop('side',axis=1)\n",
    "    return events"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SNIPPET 3.7 EXPANDING getBins TO INCORPORATE META-LABELING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getBins(events,close):\n",
    "    '''\n",
    "    Compute event's outcome (including side information, if provided).\n",
    "    events is a DataFrame where:\n",
    "    ‚Äîevents.index is event's starttime\n",
    "    ‚Äîevents[‚Äôt1‚Äô] is event's endtime\n",
    "    ‚Äîevents[‚Äôtrgt‚Äô] is event's target\n",
    "    ‚Äîevents[‚Äôside‚Äô] (optional) implies the algo's position side\n",
    "    Case 1: (‚Äôside‚Äô not in events): bin in (-1,1) <‚Äîlabel by price action\n",
    "    Case 2: (‚Äôside‚Äô in events): bin in (0,1) <‚Äîlabel by pnl (meta-labeling)\n",
    "    '''\n",
    "    #1) prices aligned with events\n",
    "    events_=events.dropna(subset=['t1'])\n",
    "    px=events_.index.union(events_['t1'].values).drop_duplicates()\n",
    "    px=close.reindex(px,method='bfill')\n",
    "    #2) create out object\n",
    "    out=pd.DataFrame(index=events_.index)\n",
    "    out['ret']=px.loc[events_['t1'].values].values/px.loc[events_.index]-1\n",
    "    if 'side' in events_:out['ret']*=events_['side'] # meta-labeling\n",
    "    out['bin']=np.sign(out['ret'])\n",
    "    if 'side' in events_:out.loc[out['ret']<=0,'bin']=0 # meta-labeling\n",
    "    return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SNIPPET 3.8 DROPPING UNDER-POPULATED LABELS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-17-0a8af544e6d2>, line 6)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-17-0a8af544e6d2>\"\u001b[1;36m, line \u001b[1;32m6\u001b[0m\n\u001b[1;33m    print 'dropped label',df0.argmin(),df0.min()\u001b[0m\n\u001b[1;37m                        ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "def dropLabels(events,minPtc=.05):\n",
    "    # apply weights, drop labels with insufficient examples\n",
    "    while True:\n",
    "        df0=events['bin'].value_counts(normalize=True)\n",
    "        if df0.min()>minPct or df0.shape[0]<3:break\n",
    "        print 'dropped label',df0.argmin(),df0.min()\n",
    "        events=events[events['bin']!=df0.argmin()]\n",
    "    return events"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CHAPTER 4 - Sample Weights\n",
    "#### SNIPPET 4.1 ESTIMATING THE UNIQUENESS OF A LABEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mpNumCoEvents(closeIdx,t1,molecule):\n",
    "    '''\n",
    "    Compute the number of concurrent events per bar.\n",
    "    +molecule[0] is the date of the first event on which the weight will be computed\n",
    "    +molecule[-1] is the date of the last event on which the weight will be computed\n",
    "    Any event that starts before t1[molecule].max() impacts the count.\n",
    "    '''\n",
    "    #1) find events that span the period [molecule[0],molecule[-1]]\n",
    "    t1=t1.fillna(closeIdx[-1]) # unclosed events still must impact other weights\n",
    "    t1=t1[t1>=molecule[0]] # events that end at or after molecule[0]\n",
    "    t1=t1.loc[:t1[molecule].max()] # events that start at or before t1[molecule].max()\n",
    "    #2) count events spanning a bar\n",
    "    iloc=closeIdx.searchsorted(np.array([t1.index[0],t1.max()]))\n",
    "    count=pd.Series(0,index=closeIdx[iloc[0]:iloc[1]+1])\n",
    "    for tIn,tOut in t1.iteritems():count.loc[tIn:tOut]+=1.\n",
    "    return count.loc[molecule[0]:t1[molecule].max()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SNIPPET 4.2 ESTIMATING THE AVERAGE UNIQUENESS OF A LABEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'mpPandasObj' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-20-54dedde9b64a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      6\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mwght\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;31m#‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m numCoEvents=mpPandasObj(mpNumCoEvents,('molecule',events.index),numThreads, \\\n\u001b[0m\u001b[0;32m      9\u001b[0m     closeIdx=close.index,t1=events['t1'])\n\u001b[0;32m     10\u001b[0m \u001b[0mnumCoEvents\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnumCoEvents\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m~\u001b[0m\u001b[0mnumCoEvents\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mduplicated\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkeep\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'last'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'mpPandasObj' is not defined"
     ]
    }
   ],
   "source": [
    "def mpSampleTW(t1,numCoEvents,molecule):\n",
    "    # Derive average uniqueness over the event's lifespan\n",
    "    wght=pd.Series(index=molecule)\n",
    "    for tIn,tOut in t1.loc[wght.index].iteritems():\n",
    "        wght.loc[tIn]=(1./numCoEvents.loc[tIn:tOut]).mean()\n",
    "    return wght\n",
    "#‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî\n",
    "numCoEvents=mpPandasObj(mpNumCoEvents,('molecule',events.index),numThreads, \\\n",
    "    closeIdx=close.index,t1=events['t1'])\n",
    "numCoEvents=numCoEvents.loc[~numCoEvents.index.duplicated(keep='last')]\n",
    "numCoEvents=numCoEvents.reindex(close.index).fillna(0)\n",
    "out['tW']=mpPandasObj(mpSampleTW,('molecule',events.index),numThreads, \\\n",
    "    t1=events['t1'],numCoEvents=numCoEvents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SNIPPET 4.3 BUILD AN INDICATOR MATRIX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd,numpy as np\n",
    "#‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî\n",
    "def getIndMatrix(barIx,t1):\n",
    "    # Get indicator matrix\n",
    "    indM=pd.DataFrame(0,index=barIx,columns=range(t1.shape[0]))\n",
    "    for i,(t0,t1) in enumerate(t1.iteritems()):indM.loc[t0:t1,i]=1.\n",
    "    return indM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SNIPPET 4.4 COMPUTE AVERAGE UNIQUENESS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getAvgUniqueness(indM):\n",
    "    # Average uniqueness from indicator matrix\n",
    "    c=indM.sum(axis=1) # concurrency\n",
    "    u=indM.div(c,axis=0) # uniqueness\n",
    "    avgU=u[u>0].mean() # average uniqueness\n",
    "    return avgU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SNIPPET 4.5 RETURN SAMPLE FROM SEQUENTIAL BOOTSTRAP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def seqBootstrap(indM,sLength=None):\n",
    "    # Generate a sample via sequential bootstrap\n",
    "    if sLength is None:sLength=indM.shape[1]\n",
    "    phi=[]\n",
    "    while len(phi)<sLength:\n",
    "        avgU=pd.Series()\n",
    "        for i in indM:\n",
    "            indM_=indM[phi+[i]] # reduce indM\n",
    "            avgU.loc[i]=getAvgUniqueness(indM_).iloc[-1]\n",
    "        prob=avgU/avgU.sum() # draw prob\n",
    "        phi+=[np.random.choice(indM.columns,p=prob)]\n",
    "    return phi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SNIPPET 4.6 EXAMPLE OF SEQUENTIAL BOOTSTRAP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "Missing parentheses in call to 'print'. Did you mean print(phi)? (<ipython-input-24-43239ebd4d9a>, line 6)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-24-43239ebd4d9a>\"\u001b[1;36m, line \u001b[1;32m6\u001b[0m\n\u001b[1;33m    print phi\u001b[0m\n\u001b[1;37m            ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m Missing parentheses in call to 'print'. Did you mean print(phi)?\n"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    t1=pd.Series([2,3,5],index=[0,2,4]) # t0,t1 for each feature obs\n",
    "    barIx=range(t1.max()+1) # index of bars\n",
    "    indM=getIndMatrix(barIx,t1)\n",
    "    phi=np.random.choice(indM.columns,size=indM.shape[1])\n",
    "    print phi\n",
    "    print 'Standard uniqueness:',getAvgUniqueness(indM[phi]).mean()\n",
    "    phi=seqBootstrap(indM)\n",
    "    print phi\n",
    "    print 'Sequential uniqueness:',getAvgUniqueness(indM[phi]).mean()\n",
    "    return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SNIPPET 4.7 GENERATING A RANDOM T1 SERIES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getRndT1(numObs,numBars,maxH):\n",
    "    # random t1 Series\n",
    "    t1=pd.Series()\n",
    "    for i in xrange(numObs):\n",
    "        ix=np.random.randint(0,numBars)\n",
    "        val=ix+np.random.randint(1,maxH)\n",
    "        t1.loc[ix]=val\n",
    "    return t1.sort_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SNIPPET 4.8 UNIQUENESS FROM STANDARD AND SEQUENTIAL BOOTSTRAPS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def auxMC(numObs,numBars,maxH):\n",
    "    # Parallelized auxiliary function\n",
    "    t1=getRndT1(numObs,numBars,maxH)\n",
    "    barIx=range(t1.max()+1)\n",
    "    indM=getIndMatrix(barIx,t1)\n",
    "    phi=np.random.choice(indM.columns,size=indM.shape[1])\n",
    "    stdU=getAvgUniqueness(indM[phi]).mean()\n",
    "    phi=seqBootstrap(indM)\n",
    "    seqU=getAvgUniqueness(indM[phi]).mean()\n",
    "    return {'stdU':stdU,'seqU':seqU}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SNIPPET 4.9 MULTI-THREADED MONTE CARLO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-27-6be60bb8789b>, line 12)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-27-6be60bb8789b>\"\u001b[1;36m, line \u001b[1;32m12\u001b[0m\n\u001b[1;33m    print pd.DataFrame(out).describe()\u001b[0m\n\u001b[1;37m           ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd,numpy as np\n",
    "from mpEngine import processJobs,processJobs_\n",
    "#‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî\n",
    "def mainMC(numObs=10,numBars=100,maxH=5,numIters=1E6,numThreads=24):\n",
    "    # Monte Carlo experiments\n",
    "    jobs=[]\n",
    "    for i in xrange(int(numIters)):\n",
    "        job={'func':auxMC,'numObs':numObs,'numBars':numBars,'maxH':maxH}\n",
    "        jobs.append(job)\n",
    "    if numThreads==1:out=processJobs_(jobs)\n",
    "    else:out=processJobs(jobs,numThreads=numThreads)\n",
    "    print pd.DataFrame(out).describe()\n",
    "    return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SNIPPET 4.10 DETERMINATION OF SAMPLE WEIGHT BY ABSOLUTE RETURN ATTRIBUTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'mpPandasObj' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-30-9c8ddb5f5741>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      7\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mwght\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mabs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;31m#‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 9\u001b[1;33m out['w']=mpPandasObj(mpSampleW,('molecule',events.index),numThreads, \\\n\u001b[0m\u001b[0;32m     10\u001b[0m     t1=events['t1'],numCoEvents=numCoEvents,close=close)\n\u001b[0;32m     11\u001b[0m \u001b[0mout\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'w'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m*=\u001b[0m\u001b[0mout\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m/\u001b[0m\u001b[0mout\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'w'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'mpPandasObj' is not defined"
     ]
    }
   ],
   "source": [
    "def mpSampleW(t1,numCoEvents,close,molecule):\n",
    "    # Derive sample weight by return attribution\n",
    "    ret=np.log(close).diff() # log-returns, so that they are additive\n",
    "    wght=pd.Series(index=molecule)\n",
    "    for tIn,tOut in t1.loc[wght.index].iteritems():\n",
    "        wght.loc[tIn]=(ret.loc[tIn:tOut]/numCoEvents.loc[tIn:tOut]).sum()\n",
    "    return wght.abs()\n",
    "#‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî\n",
    "out['w']=mpPandasObj(mpSampleW,('molecule',events.index),numThreads, \\\n",
    "    t1=events['t1'],numCoEvents=numCoEvents,close=close)\n",
    "out['w']*=out.shape[0]/out['w'].sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SNIPPET 4.11 IMPLEMENTATION OF TIME-DECAY FACTORS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "Missing parentheses in call to 'print'. Did you mean print(const,slope)? (<ipython-input-31-529a0bdafba1>, line 10)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-31-529a0bdafba1>\"\u001b[1;36m, line \u001b[1;32m10\u001b[0m\n\u001b[1;33m    print const,slope\u001b[0m\n\u001b[1;37m              ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m Missing parentheses in call to 'print'. Did you mean print(const,slope)?\n"
     ]
    }
   ],
   "source": [
    "def getTimeDecay(tW,clfLastW=1.):\n",
    "    # apply piecewise-linear decay to observed uniqueness (tW)\n",
    "    # newest observation gets weight=1, oldest observation gets weight=clfLastW\n",
    "    clfW=tW.sort_index().cumsum()\n",
    "    if clfLastW>=0:slope=(1.-clfLastW)/clfW.iloc[-1]\n",
    "    else:slope=1./((clfLastW+1)*clfW.iloc[-1])\n",
    "    const=1.-slope*clfW.iloc[-1]\n",
    "    clfW=const+slope*clfW\n",
    "    clfW[clfW<0]=0\n",
    "    print const,slope\n",
    "    return clfW"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CHAPTER 5 - Fractionally Differentiated Features\n",
    "#### SNIPPET 5.1 WEIGHTING FUNCTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'mpl' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-32-a1065cd0ee94>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[1;31m#‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî-\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0m__name__\u001b[0m\u001b[1;33m==\u001b[0m\u001b[1;34m'__main__'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 21\u001b[1;33m     \u001b[0mplotWeights\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdRange\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mnPlots\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m11\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0msize\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m6\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     22\u001b[0m     \u001b[0mplotWeights\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdRange\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mnPlots\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m11\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0msize\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m6\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-32-a1065cd0ee94>\u001b[0m in \u001b[0;36mplotWeights\u001b[1;34m(dRange, nPlots, size)\u001b[0m\n\u001b[0;32m     15\u001b[0m         \u001b[0mw\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mw\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mw_\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mhow\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'outer'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m     \u001b[0max\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mw\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 17\u001b[1;33m     \u001b[0max\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlegend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mloc\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'upper left'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m;\u001b[0m\u001b[0mmpl\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     18\u001b[0m     \u001b[1;32mreturn\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[1;31m#‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî-\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'mpl' is not defined"
     ]
    }
   ],
   "source": [
    "def getWeights(d,size):\n",
    "    # thres>0 drops insignificant weights\n",
    "    w=[1.]\n",
    "    for k in range(1,size):\n",
    "        w_=-w[-1]/k*(d-k+1)\n",
    "        w.append(w_)\n",
    "    w=np.array(w[::-1]).reshape(-1,1)\n",
    "    return w\n",
    "#‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî-\n",
    "def plotWeights(dRange,nPlots,size):\n",
    "    w=pd.DataFrame()\n",
    "    for d in np.linspace(dRange[0],dRange[1],nPlots):\n",
    "        w_=getWeights(d,size=size)\n",
    "        w_=pd.DataFrame(w_,index=range(w_.shape[0])[::-1],columns=[d])\n",
    "        w=w.join(w_,how='outer')\n",
    "    ax=w.plot()\n",
    "    ax.legend(loc='upper left');mpl.show()\n",
    "    return\n",
    "#‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî-\n",
    "if __name__=='__main__':\n",
    "    plotWeights(dRange=[0,1],nPlots=11,size=6)\n",
    "    plotWeights(dRange=[1,2],nPlots=11,size=6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SNIPPET 5.2 STANDARD FRACDIFF (EXPANDING WINDOW)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fracDiff(series,d,thres=.01):\n",
    "    '''\n",
    "    Increasing width window, with treatment of NaNs\n",
    "    Note 1: For thres=1, nothing is skipped.\n",
    "    Note 2: d can be any positive fractional, not necessarily bounded [0,1].\n",
    "    '''\n",
    "    #1) Compute weights for the longest series\n",
    "    w=getWeights(d,series.shape[0])\n",
    "    #2) Determine initial calcs to be skipped based on weight-loss threshold\n",
    "    w_=np.cumsum(abs(w))\n",
    "    w_/=w_[-1]\n",
    "    skip=w_[w_>thres].shape[0]\n",
    "    #3) Apply weights to values\n",
    "    df={}\n",
    "    for name in series.columns:\n",
    "        seriesF,df_=series[[name]].fillna(method='ffill').dropna(),pd.Series()\n",
    "        for iloc in range(skip,seriesF.shape[0]):\n",
    "            loc=seriesF.index[iloc]\n",
    "            if not np.isfinite(series.loc[loc,name]):continue # exclude NAs\n",
    "            df_[loc]=np.dot(w[-(iloc+1):,:].T,seriesF.loc[:loc])[0,0]\n",
    "        df[name]=df_.copy(deep=True)\n",
    "    df=pd.concat(df,axis=1)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SNIPPET 5.3 THE NEW FIXED-WIDTH WINDOWFRACDIFF METHOD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fracDiff_FFD(series,d,thres=1e-5):\n",
    "    '''\n",
    "    Constant width window (new solution)\n",
    "    Note 1: thres determines the cut-off weight for the window\n",
    "    Note 2: d can be any positive fractional, not necessarily bounded [0,1].\n",
    "    '''\n",
    "    #1) Compute weights for the longest series\n",
    "    w=getWeights_FFD(d,thres)\n",
    "    width=len(w)-1\n",
    "    #2) Apply weights to values\n",
    "    df={}\n",
    "    for name in series.columns:\n",
    "        seriesF,df_=series[[name]].fillna(method='ffill').dropna(),pd.Series()\n",
    "        for iloc1 in range(width,seriesF.shape[0]):\n",
    "            loc0,loc1=seriesF.index[iloc1-width],seriesF.index[iloc1]\n",
    "            if not np.isfinite(series.loc[loc1,name]):continue # exclude NAs\n",
    "            df_[loc1]=np.dot(w.T,seriesF.loc[loc0:loc1])[0,0]\n",
    "        df[name]=df_.copy(deep=True)\n",
    "    df=pd.concat(df,axis=1)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SNIPPET 5.4 FINDING THE MINIMUM D VALUE THAT PASSES THE ADF TEST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plotMinFFD():\n",
    "    from statsmodels.tsa.stattools import adfuller\n",
    "    path,instName='./','ES1_Index_Method12'\n",
    "    out=pd.DataFrame(columns=['adfStat','pVal','lags','nObs','95% conf','corr'])\n",
    "    df0=pd.read_csv(path+instName+'.csv',index_col=0,parse_dates=True)\n",
    "    for d in np.linspace(0,1,11):\n",
    "        df1=np.log(df0[['Close']]).resample('1D').last() # downcast to daily obs\n",
    "        df2=fracDiff_FFD(df1,d,thres=.01)\n",
    "        corr=np.corrcoef(df1.loc[df2.index,'Close'],df2['Close'])[0,1]\n",
    "        df2=adfuller(df2['Close'],maxlag=1,regression='c',autolag=None)\n",
    "        out.loc[d]=list(df2[:4])+[df2[4]['5%']]+[corr] # with critical value\n",
    "    out.to_csv(path+instName+'_testMinFFD.csv')\n",
    "    out[['adfStat','corr']].plot(secondary_y='adfStat')\n",
    "    mpl.axhline(out['95% conf'].mean(),linewidth=1,color='r',linestyle='dotted')\n",
    "    mpl.savefig(path+instName+'_testMinFFD.png')\n",
    "    return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CHAPTER 6 - Ensemble Methods\n",
    "#### SNIPPET 6.1 ACCURACY OF THE BAGGING CLASSIFIER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "Missing parentheses in call to 'print'. Did you mean print(p,1-p_)? (<ipython-input-36-ec1d0d032a96>, line 6)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-36-ec1d0d032a96>\"\u001b[1;36m, line \u001b[1;32m6\u001b[0m\n\u001b[1;33m    print p,1-p_\u001b[0m\n\u001b[1;37m          ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m Missing parentheses in call to 'print'. Did you mean print(p,1-p_)?\n"
     ]
    }
   ],
   "source": [
    "from scipy.misc import comb\n",
    "N,p,k=100,1./3,3.\n",
    "p_=0\n",
    "for i in xrange(0,int(N/k)+1):\n",
    "    p_+=comb(N,i)*p**i*(1-p)**(N-i)\n",
    "print p,1-p_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SNIPPET 6.2 THREE WAYS OF SETTING UP AN RF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'RandomForestClassifier' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-37-e9b94b7c5404>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m clf0=RandomForestClassifier(n_estimators=1000,class_weight='balanced_subsample',\n\u001b[0m\u001b[0;32m      2\u001b[0m     criterion='entropy')\n\u001b[0;32m      3\u001b[0m clf1=DecisionTreeClassifier(criterion='entropy',max_features='auto',\n\u001b[0;32m      4\u001b[0m     class_weight='balanced')\n\u001b[0;32m      5\u001b[0m \u001b[0mclf1\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mBaggingClassifier\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbase_estimator\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mclf1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mn_estimators\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1000\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mmax_samples\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mavgU\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'RandomForestClassifier' is not defined"
     ]
    }
   ],
   "source": [
    "clf0=RandomForestClassifier(n_estimators=1000,class_weight='balanced_subsample',\n",
    "    criterion='entropy')\n",
    "clf1=DecisionTreeClassifier(criterion='entropy',max_features='auto',\n",
    "    class_weight='balanced')\n",
    "clf1=BaggingClassifier(base_estimator=clf1,n_estimators=1000,max_samples=avgU)\n",
    "clf2=RandomForestClassifier(n_estimators=1,criterion='entropy',bootstrap=False,\n",
    "    class_weight='balanced_subsample')\n",
    "clf2=BaggingClassifier(base_estimator=clf2,n_estimators=1000,max_samples=avgU,\n",
    "    max_features=1.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CHAPTER 7 - Cross-Validation in Finance\n",
    "#### SNIPPET 7.1 PURGING OBSERVATION IN THE TRAINING SET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getTrainTimes(t1,testTimes):\n",
    "    '''\n",
    "    Given testTimes, find the times of the training observations.\n",
    "    ‚Äît1.index: Time when the observation started.\n",
    "    ‚Äît1.value: Time when the observation ended.\n",
    "    ‚ÄîtestTimes: Times of testing observations.\n",
    "    '''\n",
    "    trn=t1.copy(deep=True)\n",
    "    for i,j in testTimes.iteritems():\n",
    "        df0=trn[(i<=trn.index)&(trn.index<=j)].index # train starts within test\n",
    "        df1=trn[(i<=trn)&(trn<=j)].index # train ends within test\n",
    "        df2=trn[(trn.index<=i)&(j<=trn)].index # train envelops test\n",
    "        trn=trn.drop(df0.union(df1).union(df2))\n",
    "    return trn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SNIPPET 7.2 EMBARGO ON TRAINING OBSERVATIONS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'mbrg' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-41-97c1a0034adc>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      9\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mmbrg\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[1;31m#‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 11\u001b[1;33m \u001b[0mtestTimes\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mSeries\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmbrg\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mdt1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mdt0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m# include embargo before purge\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     12\u001b[0m \u001b[0mtrainTimes\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mgetTrainTimes\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mt1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtestTimes\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[0mtestTimes\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mt1\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mdt0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mdt1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'mbrg' is not defined"
     ]
    }
   ],
   "source": [
    "def getEmbargoTimes(times,pctEmbargo):\n",
    "    # Get embargo time for each bar\n",
    "    step=int(times.shape[0]*pctEmbargo)\n",
    "    if step==0:\n",
    "        mbrg=pd.Series(times,index=times)\n",
    "    else:\n",
    "        mbrg=pd.Series(times[step:],index=times[:-step])\n",
    "        mbrg=mbrg.append(pd.Series(times[-1],index=times[-step:]))\n",
    "    return mbrg\n",
    "#‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî\n",
    "testTimes=pd.Series(mbrg[dt1],index=[dt0]) # include embargo before purge\n",
    "trainTimes=getTrainTimes(t1,testTimes)\n",
    "testTimes=t1.loc[dt0:dt1].index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SNIPPET 7.3 CROSS-VALIDATION CLASS WHEN OBSERVATIONS OVERLAP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name '_BaseKFold' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-42-8e067ea260c7>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mclass\u001b[0m \u001b[0mPurgedKFold\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_BaseKFold\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m     '''\n\u001b[0;32m      3\u001b[0m     \u001b[0mExtend\u001b[0m \u001b[0mKFold\u001b[0m \u001b[1;32mclass\u001b[0m \u001b[0mto\u001b[0m \u001b[0mwork\u001b[0m \u001b[1;32mwith\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0mthat\u001b[0m \u001b[0mspan\u001b[0m \u001b[0mintervals\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[0mThe\u001b[0m \u001b[0mtrain\u001b[0m \u001b[1;32mis\u001b[0m \u001b[0mpurged\u001b[0m \u001b[0mof\u001b[0m \u001b[0mobservations\u001b[0m \u001b[0moverlapping\u001b[0m \u001b[0mtest\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0mlabel\u001b[0m \u001b[0mintervals\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[0mTest\u001b[0m \u001b[0mset\u001b[0m \u001b[1;32mis\u001b[0m \u001b[0massumed\u001b[0m \u001b[0mcontiguous\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mw\u001b[0m\u001b[1;33m/\u001b[0m\u001b[0mo\u001b[0m \u001b[0mtraining\u001b[0m \u001b[0msamples\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mbetween\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name '_BaseKFold' is not defined"
     ]
    }
   ],
   "source": [
    "class PurgedKFold(_BaseKFold):\n",
    "    '''\n",
    "    Extend KFold class to work with labels that span intervals\n",
    "    The train is purged of observations overlapping test-label intervals\n",
    "    Test set is assumed contiguous (shuffle=False), w/o training samples in between\n",
    "    '''\n",
    "    def __init__(self,n_splits=3,t1=None,pctEmbargo=0.):\n",
    "        if not isinstance(t1,pd.Series):\n",
    "            raise ValueError('Label Through Dates must be a pd.Series')\n",
    "        super(PurgedKFold,self).__init__(n_splits,shuffle=False,random_state=None)\n",
    "        self.t1=t1\n",
    "        self.pctEmbargo=pctEmbargo\n",
    "    def split(self,X,y=None,groups=None):\n",
    "        if (X.index==self.t1.index).sum()!=len(self.t1):\n",
    "            raise ValueError('X and ThruDateValues must have the same index')\n",
    "        indices=np.arange(X.shape[0])\n",
    "        mbrg=int(X.shape[0]*self.pctEmbargo)\n",
    "        test_starts=[(i[0],i[-1]+1) for i in \\\n",
    "            np.array_split(np.arange(X.shape[0]),self.n_splits)]\n",
    "        for i,j in test_starts:\n",
    "            t0=self.t1.index[i] # start of test set\n",
    "            test_indices=indices[i:j]\n",
    "            maxT1Idx=self.t1.index.searchsorted(self.t1[test_indices].max())\n",
    "            train_indices=self.t1.index.searchsorted(self.t1[self.t1<=t0].index)\n",
    "            if maxT1Idx<X.shape[0]: # right train (with embargo)\n",
    "                train_indices=np.concatenate((train_indices,indices[maxT1Idx+mbrg:]))\n",
    "            yield train_indices,test_indices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SNIPPET 7.4 USING THE PurgedKFold CLASS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cvScore(clf,X,y,sample_weight,scoring='neg_log_loss',t1=None,cv=None,cvGen=None,pctEmbargo=None):\n",
    "    if scoring not in ['neg_log_loss','accuracy']:\n",
    "        raise Exception('wrong scoring method.')\n",
    "    from sklearn.metrics import log_loss,accuracy_score\n",
    "    from clfSequential import PurgedKFold\n",
    "    if cvGen is None:\n",
    "        cvGen=PurgedKFold(n_splits=cv,t1=t1,pctEmbargo=pctEmbargo) # purged\n",
    "    score=[]\n",
    "    for train,test in cvGen.split(X=X):\n",
    "        fit=clf.fit(X=X.iloc[train,:],y=y.iloc[train],sample_weight=sample_weight.iloc[train].values)\n",
    "        if scoring=='neg_log_loss':\n",
    "            prob=fit.predict_proba(X.iloc[test,:])\n",
    "            score_=-log_loss(y.iloc[test],prob,sample_weight=sample_weight.iloc[test].values,labels=clf.classes_)\n",
    "        else:\n",
    "            pred=fit.predict(X.iloc[test,:])\n",
    "            score_=accuracy_score(y.iloc[test],pred,sample_weight= \\\n",
    "                sample_weight.iloc[test].values)\n",
    "        score.append(score_)\n",
    "    return np.array(score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CHAPTER 8 - Feature Importance\n",
    "\n",
    "#### SNIPPET 8.2 MDI FEATURE IMPORTANCE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def featImpMDI(fit,featNames):\n",
    "    # feat importance based on IS mean impurity reduction\n",
    "    df0={i:tree.feature_importances_ for i,tree in enumerate(fit.estimators_)}\n",
    "    df0=pd.DataFrame.from_dict(df0,orient='index')\n",
    "    df0.columns=featNames\n",
    "    df0=df0.replace(0,np.nan) # because max_features=1\n",
    "    imp=pd.concat({'mean':df0.mean(),'std':df0.std()*df0.shape[0]**-.5},axis=1)\n",
    "    imp/=imp['mean'].sum()\n",
    "    return imp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SNIPPET 8.3 MDA FEATURE IMPORTANCE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def featImpMDA(clf,X,y,cv,sample_weight,t1,pctEmbargo,scoring='neg_log_loss'):\n",
    "    # feat importance based on OOS score reduction\n",
    "    if scoring not in ['neg_log_loss','accuracy']:\n",
    "        raise Exception('wrong scoring method.')\n",
    "    from sklearn.metrics import log_loss,accuracy_score\n",
    "    cvGen=PurgedKFold(n_splits=cv,t1=t1,pctEmbargo=pctEmbargo) # purged cv\n",
    "    scr0,scr1=pd.Series(),pd.DataFrame(columns=X.columns)\n",
    "    for i,(train,test) in enumerate(cvGen.split(X=X)):\n",
    "        X0,y0,w0=X.iloc[train,:],y.iloc[train],sample_weight.iloc[train]\n",
    "        X1,y1,w1=X.iloc[test,:],y.iloc[test],sample_weight.iloc[test]\n",
    "        fit=clf.fit(X=X0,y=y0,sample_weight=w0.values)\n",
    "        if scoring=='neg_log_loss':\n",
    "            prob=fit.predict_proba(X1)\n",
    "            scr0.loc[i]=-log_loss(y1,prob,sample_weight=w1.values,\n",
    "                labels=clf.classes_)\n",
    "        else:\n",
    "            pred=fit.predict(X1)\n",
    "            scr0.loc[i]=accuracy_score(y1,pred,sample_weight=w1.values)\n",
    "        for j in X.columns:\n",
    "            X1_=X1.copy(deep=True)\n",
    "            np.random.shuffle(X1_[j].values) # permutation of a single column\n",
    "            if scoring=='neg_log_loss':\n",
    "                prob=fit.predict_proba(X1_)\n",
    "                scr1.loc[i,j]=-log_loss(y1,prob,sample_weight=w1.values,\n",
    "                    labels=clf.classes_)\n",
    "            else:\n",
    "                pred=fit.predict(X1_)\n",
    "                scr1.loc[i,j]=accuracy_score(y1,pred,sample_weight=w1.values)\n",
    "            imp=(-scr1).add(scr0,axis=0)\n",
    "    if scoring=='neg_log_loss':imp=imp/-scr1\n",
    "    else:imp=imp/(1.-scr1)\n",
    "    imp=pd.concat({'mean':imp.mean(),'std':imp.std()*imp.shape[0]**-.5},axis=1)\n",
    "    return imp,scr0.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SNIPPET 8.4 IMPLEMENTATION OF SFI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def auxFeatImpSFI(featNames,clf,trnsX,cont,scoring,cvGen):\n",
    "    imp=pd.DataFrame(columns=['mean','std'])\n",
    "    for featName in featNames:\n",
    "        df0=cvScore(clf,X=trnsX[[featName]],y=cont['bin'],sample_weight=cont['w'],\n",
    "            scoring=scoring,cvGen=cvGen)\n",
    "        imp.loc[featName,'mean']=df0.mean()\n",
    "        imp.loc[featName,'std']=df0.std()*df0.shape[0]**-.5\n",
    "    return imp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SNIPPET 8.5 COMPUTATION OF ORTHOGONAL FEATURES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_eVec(dot,varThres):\n",
    "    # compute eVec from dot prod matrix, reduce dimension\n",
    "    eVal,eVec=np.linalg.eigh(dot)\n",
    "    idx=eVal.argsort()[::-1] # arguments for sorting eVal desc\n",
    "    eVal,eVec=eVal[idx],eVec[:,idx]\n",
    "    #2) only positive eVals\n",
    "    eVal=pd.Series(eVal,index=['PC_'+str(i+1) for i in range(eVal.shape[0])])\n",
    "    eVec=pd.DataFrame(eVec,index=dot.index,columns=eVal.index)\n",
    "    eVec=eVec.loc[:,eVal.index]\n",
    "    #3) reduce dimension, form PCs\n",
    "    cumVar=eVal.cumsum()/eVal.sum()\n",
    "    dim=cumVar.values.searchsorted(varThres)\n",
    "    eVal,eVec=eVal.iloc[:dim+1],eVec.iloc[:,:dim+1]\n",
    "    return eVal,eVec\n",
    "#-----------------------------------------------------------------\n",
    "def orthoFeats(dfX,varThres=.95):\n",
    "    # Given a dataframe dfX of features, compute orthofeatures dfP\n",
    "    dfZ=dfX.sub(dfX.mean(),axis=1).div(dfX.std(),axis=1) # standardize\n",
    "    dot=pd.DataFrame(np.dot(dfZ.T,dfZ),index=dfX.columns,columns=dfX.columns)\n",
    "    eVal,eVec=get_eVec(dot,varThres)\n",
    "    dfP=np.dot(dfZ,eVec)\n",
    "    return dfP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SNIPPET 8.6 COMPUTATION OF WEIGHTED KENDALL‚ÄôS TAU BETWEEN FEATURE IMPORTANCE AND INVERSE PCA RANKING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8133333333333331"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    ">>> import numpy as np\n",
    ">>> from scipy.stats import weightedtau\n",
    ">>> featImp=np.array([.55,.33,.07,.05]) # feature importance\n",
    ">>> pcRank=np.array([1,2,4,3]) # PCA rank\n",
    ">>> weightedtau(featImp,pcRank**-1.)[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SNIPPET 8.7 CREATING A SYNTHETIC DATASET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "def getTestData(n_features=40,n_informative=10,n_redundant=10,n_samples=10000):\n",
    "    # generate a random dataset for a classification problem\n",
    "    from sklearn.datasets import make_classification\n",
    "    trnsX,cont=make_classification(n_samples=n_samples,n_features=n_features,\n",
    "        n_informative=n_informative,n_redundant=n_redundant,random_state=0,shuffle=False)\n",
    "    df0=pd.DatetimeIndex(periods=n_samples,freq=pd.tseries.offsets.BDay(),end=pd.datetime.today())\n",
    "    trnsX,cont=pd.DataFrame(trnsX,index=df0),pd.Series(cont,index=df0).to_frame('bin')\n",
    "    df0=['I_'+str(i) for i in xrange(n_informative)]+['R_'+str(i) for i in xrange(n_redundant)]\n",
    "    df0+=['N_'+str(i) for i in xrange(n_features-len(df0))]\n",
    "    trnsX.columns=df0\n",
    "    cont['w']=1./cont.shape[0]\n",
    "    cont['t1']=pd.Series(cont.index,index=cont.index)\n",
    "    return trnsX,cont"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SNIPPET 8.8 CALLING FEATURE IMPORTANCE FOR ANY METHOD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def featImportance(trnsX,cont,n_estimators=1000,cv=10,max_samples=1.,numThreads=24,\n",
    "                pctEmbargo=0,scoring='accuracy',method='SFI',minWLeaf=0.,**kargs):\n",
    "    # feature importance from a random forest\n",
    "    from sklearn.tree import DecisionTreeClassifier\n",
    "    from sklearn.ensemble import BaggingClassifier\n",
    "    from mpEngine import mpPandasObj\n",
    "    n_jobs=(-1 if numThreads>1 else 1) # run 1 thread with ht_helper in dirac1\n",
    "    #1) prepare classifier,cv. max_features=1, to prevent masking\n",
    "    clf=DecisionTreeClassifier(criterion='entropy',max_features=1,\n",
    "        class_weight='balanced',min_weight_fraction_leaf=minWLeaf)\n",
    "    clf=BaggingClassifier(base_estimator=clf,n_estimators=n_estimators,\n",
    "        max_features=1.,max_samples=max_samples,oob_score=True,n_jobs=n_jobs)\n",
    "    fit=clf.fit(X=trnsX,y=cont['bin'],sample_weight=cont['w'].values)\n",
    "    oob=fit.oob_score_\n",
    "    if method=='MDI':\n",
    "        imp=featImpMDI(fit,featNames=trnsX.columns)\n",
    "        oos=cvScore(clf,X=trnsX,y=cont['bin'],cv=cv,sample_weight=cont['w'],\n",
    "            t1=cont['t1'],pctEmbargo=pctEmbargo,scoring=scoring).mean()\n",
    "    elif method=='MDA':\n",
    "        imp,oos=featImpMDA(clf,X=trnsX,y=cont['bin'],cv=cv,sample_weight=cont['w'],\n",
    "            t1=cont['t1'],pctEmbargo=pctEmbargo,scoring=scoring)\n",
    "    elif method=='SFI':\n",
    "        cvGen=PurgedKFold(n_splits=cv,t1=cont['t1'],pctEmbargo=pctEmbargo)\n",
    "        oos=cvScore(clf,X=trnsX,y=cont['bin'],sample_weight=cont['w'],scoring=scoring,\n",
    "            cvGen=cvGen).mean()\n",
    "        clf.n_jobs=1 # paralellize auxFeatImpSFI rather than clf\n",
    "        imp=mpPandasObj(auxFeatImpSFI,('featNames',trnsX.columns),numThreads,\n",
    "            clf=clf,trnsX=trnsX,cont=cont,scoring=scoring,cvGen=cvGen)\n",
    "    return imp,oob,oos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SNIPPET 8.9 CALLING ALL COMPONENTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def testFunc(n_features=40,n_informative=10,n_redundant=10,n_estimators=1000,\n",
    "            n_samples=10000,cv=10):\n",
    "    # test the performance of the feat importance functions on artificial data\n",
    "    # Nr noise features = n_features‚Äîn_informative‚Äîn_redundant\n",
    "    trnsX,cont=getTestData(n_features,n_informative,n_redundant,n_samples)\n",
    "    dict0={'minWLeaf':[0.],'scoring':['accuracy'],'method':['MDI','MDA','SFI'],\n",
    "        'max_samples':[1.]}\n",
    "    jobs,out=(dict(izip(dict0,i)) for i in product(*dict0.values())),[]\n",
    "    kargs={'pathOut':'./testFunc/','n_estimators':n_estimators,\n",
    "        'tag':'testFunc','cv':cv}\n",
    "    for job in jobs:\n",
    "        job['simNum']=job['method']+'_'+job['scoring']+'_'+'%.2f'%job['minWLeaf']+ \\\n",
    "            '_'+str(job['max_samples'])\n",
    "        print(job['simNum'])\n",
    "        kargs.update(job)\n",
    "        imp,oob,oos=featImportance(trnsX=trnsX,cont=cont,**kargs)\n",
    "        plotFeatImportance(imp=imp,oob=oob,oos=oos,**kargs)\n",
    "        df0=imp[['mean']]/imp['mean'].abs().sum()\n",
    "        df0['type']=[i[0] for i in df0.index]\n",
    "        df0=df0.groupby('type')['mean'].sum().to_dict()\n",
    "        df0.update({'oob':oob,'oos':oos});df0.update(job)\n",
    "        out.append(df0)\n",
    "    out=pd.DataFrame(out).sort_values(['method','scoring','minWLeaf','max_samples'])\n",
    "    out=out['method','scoring','minWLeaf','max_samples','I','R','N','oob','oos']\n",
    "    out.to_csv(kargs['pathOut']+'stats.csv')\n",
    "    return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SNIPPET 8.10 FEATURE IMPORTANCE PLOTTING FUNCTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plotFeatImportance(pathOut,imp,oob,oos,method,tag=0,simNum=0,**kargs):\n",
    "    # plot mean imp bars with std\n",
    "    mpl.figure(figsize=(10,imp.shape[0]/5.))\n",
    "    imp=imp.sort_values('mean',ascending=True)\n",
    "    ax=imp['mean'].plot(kind='barh',color='b',alpha=.25,xerr=imp['std'],\n",
    "        error_kw={'ecolor':'r'})\n",
    "    if method=='MDI':\n",
    "        mpl.xlim([0,imp.sum(axis=1).max()])\n",
    "        mpl.axvline(1./imp.shape[0],linewidth=1,color='r',linestyle='dotted')\n",
    "    ax.get_yaxis().set_visible(False)\n",
    "    for i,j in zip(ax.patches,imp.index):ax.text(i.get_width()/2,\n",
    "        i.get_y()+i.get_height()/2,j,ha='center',va='center',\n",
    "        color='black')\n",
    "    mpl.title('tag='+tag+' | simNum='+str(simNum)+' | oob='+str(round(oob,4))+\n",
    "        ' | oos='+str(round(oos,4)))\n",
    "    mpl.savefig(pathOut+'featImportance_'+str(simNum)+'.png',dpi=100)\n",
    "    mpl.clf();mpl.close()\n",
    "    return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CHAPTER 9 - Hyper-Parameter Tuning with Cross-Validation\n",
    "\n",
    "#### SNIPPET 9.1 GRID SEARCH WITH PURGED K-FOLD CROSS-VALIDATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clfHyperFit(feat,lbl,t1,pipe_clf,param_grid,cv=3,bagging=[0,None,1.],\n",
    "        n_jobs=-1,pctEmbargo=0,**fit_params):\n",
    "    if set(lbl.values)=={0,1}:scoring='f1' # f1 for meta-labeling\n",
    "    else:scoring='neg_log_loss' # symmetric towards all cases\n",
    "    #1) hyperparameter search, on train data\n",
    "    inner_cv=PurgedKFold(n_splits=cv,t1=t1,pctEmbargo=pctEmbargo) # purged\n",
    "    gs=GridSearchCV(estimator=pipe_clf,param_grid=param_grid,\n",
    "        scoring=scoring,cv=inner_cv,n_jobs=n_jobs,iid=False)\n",
    "    gs=gs.fit(feat,lbl,**fit_params).best_estimator_ # pipeline\n",
    "    #2) fit validated model on the entirety of the data\n",
    "    if bagging[1]>0:\n",
    "        gs=BaggingClassifier(base_estimator=MyPipeline(gs.steps),\n",
    "            n_estimators=int(bagging[0]),max_samples=float(bagging[1]),\n",
    "            max_features=float(bagging[2]),n_jobs=n_jobs)\n",
    "        gs=gs.fit(feat,lbl,sample_weight=fit_params \\\n",
    "            [gs.base_estimator.steps[-1][0]+'__sample_weight'])\n",
    "        gs=Pipeline([('bag',gs)])\n",
    "    return gs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SNIPPET 9.2 AN ENHANCED PIPELINE CLASS\n",
    "\n",
    "If you are not familiar with this technique for expanding classes, you may want\n",
    "to read this introductory Stackoverflow post: http://stackoverflow.com/questions/\n",
    "576169/understanding-python-super-with-init-methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Pipeline' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-63-9d4f915f143e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mclass\u001b[0m \u001b[0mMyPipeline\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mPipeline\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0msample_weight\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m             \u001b[0mfit_params\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msteps\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;34m'__sample_weight'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mMyPipeline\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'Pipeline' is not defined"
     ]
    }
   ],
   "source": [
    "class MyPipeline(Pipeline):\n",
    "    def fit(self,X,y,sample_weight=None,**fit_params):\n",
    "        if sample_weight is not None:\n",
    "            fit_params[self.steps[-1][0]+'__sample_weight']=sample_weight\n",
    "        return super(MyPipeline,self).fit(X,y,**fit_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SNIPPET 9.3 RANDOMIZED SEARCH WITH PURGED K-FOLD CV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clfHyperFit(feat,lbl,t1,pipe_clf,param_grid,cv=3,bagging=[0,None,1.],\n",
    "        rndSearchIter=0,n_jobs=-1,pctEmbargo=0,**fit_params):\n",
    "    if set(lbl.values)=={0,1}:scoring='f1' # f1 for meta-labeling\n",
    "    else:scoring='neg_log_loss' # symmetric towards all cases\n",
    "    #1) hyperparameter search, on train data\n",
    "    inner_cv=PurgedKFold(n_splits=cv,t1=t1,pctEmbargo=pctEmbargo) # purged\n",
    "    if rndSearchIter==0:\n",
    "        gs=GridSearchCV(estimator=pipe_clf,param_grid=param_grid,\n",
    "            scoring=scoring,cv=inner_cv,n_jobs=n_jobs,iid=False)\n",
    "    else:\n",
    "        gs=RandomizedSearchCV(estimator=pipe_clf,param_distributions= \\\n",
    "            param_grid,scoring=scoring,cv=inner_cv,n_jobs=n_jobs,\n",
    "            iid=False,n_iter=rndSearchIter)\n",
    "    gs=gs.fit(feat,lbl,**fit_params).best_estimator_ # pipeline\n",
    "    #2) fit validated model on the entirety of the data\n",
    "    if bagging[1]>0:\n",
    "        gs=BaggingClassifier(base_estimator=MyPipeline(gs.steps),\n",
    "            n_estimators=int(bagging[0]),max_samples=float(bagging[1]),\n",
    "            max_features=float(bagging[2]),n_jobs=n_jobs)\n",
    "        gs=gs.fit(feat,lbl,sample_weight=fit_params \\\n",
    "            [gs.base_estimator.steps[-1][0]+'__sample_weight'])\n",
    "        gs=Pipeline([('bag',gs)])\n",
    "    return gs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SNIPPET 9.4 THE logUniform_gen CLASS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KstestResult(statistic=0.010238370882811432, pvalue=0.24531920706916457)\n",
      "count    10000.000000\n",
      "mean        76.227894\n",
      "std        181.827845\n",
      "min          0.001001\n",
      "25%          0.033989\n",
      "50%          1.038381\n",
      "75%         33.702220\n",
      "max        999.402717\n",
      "dtype: float64\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np,pandas as pd,matplotlib.pyplot as mpl\n",
    "from scipy.stats import rv_continuous,kstest\n",
    "#‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî\n",
    "class logUniform_gen(rv_continuous):\n",
    "    # random numbers log-uniformly distributed between 1 and e\n",
    "    def _cdf(self,x):\n",
    "        return np.log(x/self.a)/np.log(self.b/self.a)\n",
    "def logUniform(a=1,b=np.exp(1)):return logUniform_gen(a=a,b=b,name='logUniform')\n",
    "#‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî\n",
    "a,b,size=1E-3,1E3,10000\n",
    "vals=logUniform(a=a,b=b).rvs(size=size)\n",
    "print(kstest(rvs=np.log(vals),cdf='uniform',args=(np.log(a),np.log(b/a)),N=size))\n",
    "print(pd.Series(vals).describe())\n",
    "mpl.subplot(121)\n",
    "pd.Series(np.log(vals)).hist()\n",
    "mpl.subplot(122)\n",
    "pd.Series(vals).hist()\n",
    "mpl.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CHAPTER 10 - Bet Sizing\n",
    "\n",
    "#### SNIPPET 10.1 FROM PROBABILITIES TO BET SIZE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getSignal(events,stepSize,prob,pred,numClasses,numThreads,**kargs):\n",
    "    # get signals from predictions\n",
    "    if prob.shape[0]==0:return pd.Series()\n",
    "    #1) generate signals from multinomial classification (one-vs-rest, OvR)\n",
    "    signal0=(prob-1./numClasses)/(prob*(1.-prob))**.5 # t-value of OvR\n",
    "    signal0=pred*(2*norm.cdf(signal0)-1) # signal=side*size\n",
    "    if 'side' in events:signal0*=events.loc[signal0.index,'side'] # meta-labeling\n",
    "    #2) compute average signal among those concurrently open\n",
    "    df0=signal0.to_frame('signal').join(events[['t1']],how='left')\n",
    "    df0=avgActiveSignals(df0,numThreads)\n",
    "    signal1=discreteSignal(signal0=df0,stepSize=stepSize)\n",
    "    return signal1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SNIPPET 10.2 BETS ARE AVERAGED AS LONG AS THEY ARE STILL ACTIVE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "def avgActiveSignals(signals,numThreads):\n",
    "    # compute the average signal among those active\n",
    "    #1) time points where signals change (either one starts or one ends)\n",
    "    tPnts=set(signals['t1'].dropna().values)\n",
    "    tPnts=tPnts.union(signals.index.values)\n",
    "    tPnts=list(tPnts);tPnts.sort()\n",
    "    out=mpPandasObj(mpAvgActiveSignals,('molecule',tPnts),numThreads,signals=signals)\n",
    "    return out\n",
    "#‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî\n",
    "def mpAvgActiveSignals(signals,molecule):\n",
    "    '''\n",
    "    At time loc, average signal among those still active.\n",
    "    Signal is active if:\n",
    "        a) issued before or at loc AND\n",
    "        b) loc before signal's endtime, or endtime is still unknown (NaT).\n",
    "    '''\n",
    "    out=pd.Series()\n",
    "    for loc in molecule:\n",
    "        df0=(signals.index.values<=loc)&((loc<signals['t1'])|pd.isnull(signals['t1']))\n",
    "        act=signals[df0].index\n",
    "        if len(act)>0:out[loc]=signals.loc[act,'signal'].mean()\n",
    "        else:out[loc]=0 # no signals active at this time\n",
    "    return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SNIPPET 10.3 SIZE DISCRETIZATION TO PREVENT OVERTRADING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "def discreteSignal(signal0,stepSize):\n",
    "    # discretize signal\n",
    "    signal1=(signal0/stepSize).round()*stepSize # discretize\n",
    "    signal1[signal1>1]=1 # cap\n",
    "    signal1[signal1<-1]=-1 # floor\n",
    "    return signal1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SNIPPET 10.4 DYNAMIC POSITION SIZE AND LIMIT PRICE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid character in identifier (<ipython-input-71-085c083900c1>, line 20)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-71-085c083900c1>\"\u001b[1;36m, line \u001b[1;32m20\u001b[0m\n\u001b[1;33m    return x**2*(m**-2‚Äì1)\u001b[0m\n\u001b[1;37m                       ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid character in identifier\n"
     ]
    }
   ],
   "source": [
    "def betSize(w,x):\n",
    "    return x*(w+x**2)**-.5\n",
    "#‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî\n",
    "def getTPos(w,f,mP,maxPos):\n",
    "    return int(betSize(w,f-mP)*maxPos)\n",
    "#‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî\n",
    "def invPrice(f,w,m):\n",
    "    return f-m*(w/(1-m**2))**.5\n",
    "#‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî\n",
    "def limitPrice(tPos,pos,f,w,maxPos):\n",
    "    sgn=(1 if tPos>=pos else -1)\n",
    "    lP=0\n",
    "    for j in xrange(abs(pos+sgn),abs(tPos+1)):\n",
    "        lP+=invPrice(f,w,j/float(maxPos))\n",
    "    lP/=tPos-pos\n",
    "    return lP\n",
    "#‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî\n",
    "def getW(x,m):\n",
    "    # 0<alpha<1\n",
    "    return x**2*(m**-2‚Äì1)\n",
    "#‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî\n",
    "def main():\n",
    "    pos,maxPos,mP,f,wParams=0,100,100,115,{'divergence':10,'m':.95}\n",
    "    w=getW(wParams['divergence'],wParams['m']) # calibrate w\n",
    "    tPos=getTPos(w,f,mP,maxPos) # get tPos\n",
    "    lP=limitPrice(tPos,pos,f,w,maxPos) # limit price for order\n",
    "    return\n",
    "#‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî\n",
    "if __name__=='__main__':main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CHAPTER 13 - Backtesting on Synthetic Data\n",
    "\n",
    "#### SNIPPET 13.1 PYTHON CODE FOR THE DETERMINATION OF OPTIMAL TRADING RULES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from random import gauss\n",
    "from itertools import product\n",
    "#‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî\n",
    "def main():\n",
    "    rPT=rSLm=np.linspace(0,10,21)\n",
    "    count=0\n",
    "    for prod_ in product([10,5,0,-5,-10],[5,10,25,50,100]):\n",
    "        count+=1\n",
    "        coeffs={'forecast':prod_[0],'hl':prod_[1],'sigma':1}\n",
    "        output=batch(coeffs,nIter=1e5,maxHP=100,rPT=rPT,rSLm=rSLm)\n",
    "    return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SNIPPET 13.2 PYTHON CODE FOR THE DETERMINATION OF OPTIMAL TRADING RULES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch(coeffs,nIter=1e5,maxHP=100,rPT=np.linspace(.5,10,20),\n",
    "    rSLm=np.linspace(.5,10,20),seed=0):\n",
    "    phi,output1=2**(-1./coeffs['hl']),[]\n",
    "    for comb_ in product(rPT,rSLm):\n",
    "        output2=[]\n",
    "        for iter_ in range(int(nIter)):\n",
    "            p,hp,count=seed,0,0\n",
    "            while True:\n",
    "                p=(1-phi)*coeffs['forecast']+phi*p+coeffs['sigma']*gauss(0,1)\n",
    "                cP=p-seed;hp+=1\n",
    "                if cP>comb_[0] or cP<-comb_[1] or hp>maxHP:\n",
    "                    output2.append(cP)\n",
    "                    break\n",
    "        mean,std=np.mean(output2),np.std(output2)\n",
    "        print(comb_[0],comb_[1],mean,std,mean/std)\n",
    "        output1.append((comb_[0],comb_[1],mean,std,mean/std))\n",
    "    return output1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CHAPTER 14 - Backtest Statistics\n",
    "\n",
    "#### SNIPPET 14.1 DERIVING THE TIMING OF BETS FROM A SERIES OF TARGET POSITIONS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tPos' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-76-abf336509d27>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# A bet takes place between flat positions or position flips\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mdf0\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtPos\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtPos\u001b[0m\u001b[1;33m==\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0mdf1\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtPos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshift\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m;\u001b[0m\u001b[0mdf1\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdf1\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mdf1\u001b[0m\u001b[1;33m!=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mbets\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdf0\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mintersection\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf1\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m# flattening\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mdf0\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtPos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mtPos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'tPos' is not defined"
     ]
    }
   ],
   "source": [
    "# A bet takes place between flat positions or position flips\n",
    "df0=tPos[tPos==0].index\n",
    "df1=tPos.shift(1);df1=df1[df1!=0].index\n",
    "bets=df0.intersection(df1) # flattening\n",
    "df0=tPos.iloc[1:]*tPos.iloc[:-1].values\n",
    "bets=bets.union(df0[df0<0].index).sort_values() # tPos flips\n",
    "if tPos.index[-1] not in bets:bets=bets.append(tPos.index[-1:]) # last bet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SNIPPET 14.2 IMPLEMENTATION OF A HOLDING PERIOD ESTIMATOR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getHoldingPeriod(tPos):\n",
    "    # Derive avg holding period (in days) using avg entry time pairing algo\n",
    "    hp,tEntry=pd.DataFrame(columns=['dT','w']),0.\n",
    "    pDiff,tDiff=tPos.diff(),(tPos.index-tPos.index[0])/np.timedelta64(1,'D')\n",
    "    for i in xrange(1,tPos.shape[0]):\n",
    "        if pDiff.iloc[i]*tPos.iloc[i-1]>=0: # increased or unchanged\n",
    "            if tPos.iloc[i]!=0:\n",
    "                tEntry=(tEntry*tPos.iloc[i-1]+tDiff[i]*pDiff.iloc[i])/tPos.iloc[i]\n",
    "        else: # decreased\n",
    "            if tPos.iloc[i]*tPos.iloc[i-1]<0: # flip\n",
    "                hp.loc[tPos.index[i],['dT','w']]=(tDiff[i]-tEntry,abs(tPos.iloc[i-1]))\n",
    "                tEntry=tDiff[i] # reset entry time\n",
    "            else:\n",
    "                hp.loc[tPos.index[i],['dT','w']]=(tDiff[i]-tEntry,abs(pDiff.iloc[i]))\n",
    "    if hp['w'].sum()>0:hp=(hp['dT']*hp['w']).sum()/hp['w'].sum()\n",
    "    else:hp=np.nan\n",
    "    return hp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SNIPPET 14.3 ALGORITHM FOR DERIVING HHI CONCENTRATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'ret' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-79-62744bc3ee0c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      6\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mhhi\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;31m#‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m \u001b[0mrHHIPos\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mgetHHI\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mret\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mret\u001b[0m\u001b[1;33m>=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m# concentration of positive returns per bet\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      9\u001b[0m \u001b[0mrHHINeg\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mgetHHI\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mret\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mret\u001b[0m\u001b[1;33m<\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m# concentration of negative returns per bet\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[0mtHHI\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mgetHHI\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mret\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgroupby\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTimeGrouper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfreq\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'M'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcount\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m# concentr. bets/month\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'ret' is not defined"
     ]
    }
   ],
   "source": [
    "def getHHI(betRet):\n",
    "    if betRet.shape[0]<=2:return np.nan\n",
    "    wght=betRet/betRet.sum()\n",
    "    hhi=(wght**2).sum()\n",
    "    hhi=(hhi-betRet.shape[0]**-1)/(1.-betRet.shape[0]**-1)\n",
    "    return hhi\n",
    "#‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî\n",
    "rHHIPos=getHHI(ret[ret>=0]) # concentration of positive returns per bet\n",
    "rHHINeg=getHHI(ret[ret<0]) # concentration of negative returns per bet\n",
    "tHHI=getHHI(ret.groupby(pd.TimeGrouper(freq='M')).count()) # concentr. bets/month"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SNIPPET 14.4 DERIVING THE SEQUENCE OF DD AND TuW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "def computeDD_TuW(series,dollars=False):\n",
    "    # compute series of drawdowns and the time under water associated with them\n",
    "    df0=series.to_frame('pnl')\n",
    "    df0['hwm']=series.expanding().max()\n",
    "    df1=df0.groupby('hwm').min().reset_index()\n",
    "    df1.columns=['hwm','min']\n",
    "    df1.index=df0['hwm'].drop_duplicates(keep='first').index # time of hwm\n",
    "    df1=df1[df1['hwm']>df1['min']] # hwm followed by a drawdown\n",
    "    if dollars:dd=df1['hwm']-df1['min']\n",
    "    else:dd=1-df1['min']/df1['hwm']\n",
    "    tuw=((df1.index[1:]-df1.index[:-1])/np.timedelta64(1,'Y')).values# in years\n",
    "    tuw=pd.Series(tuw,index=df1.index[:-1])\n",
    "    return dd,tuw"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CHAPTER 15 - Understanding Strategy Risk\n",
    "\n",
    "#### SNIPPET 15.1 TARGETING A SHARPE RATIO AS A FUNCTION OF THE NUMBER OF BETS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.100526 0.9949344316707509 0.10103781395040373\n"
     ]
    }
   ],
   "source": [
    "out,p=[],.55\n",
    "for i in range(1000000):\n",
    "    rnd=np.random.binomial(n=1,p=p)\n",
    "    x=(1 if rnd==1 else -1)\n",
    "    out.append(x)\n",
    "print(np.mean(out),np.std(out),np.mean(out)/np.std(out))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SNIPPET 15.2 USING THE SymPy LIBRARY FOR SYMBOLIC OPERATIONS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAALwAAAAaCAYAAAANDDZYAAAABHNCSVQICAgIfAhkiAAABWFJREFUeJztm12IVkUYx39tm+6ygrGFfSyVRGlFVmpFWdmWEvRFRt54kbwW6EXFQheVkrR9QKBFtkJBgWwWlWBZSW1pYCVdZB8uFe1Fmau0bVqBZrSx5r5dPM9xz56dOZ9z3vNm5wcvs8xzZuZ5//PszDxzdqGkpKTkP8Zy4HPgD+BXYDNwYaEelZTkyAfAEiTIZwCbgF+A1iKdKglnPbAfaClg7DOBKvBmAWPHYTbi390xn58EHAFuDXmmSL3zJqleNedSYAS4v6DxFyACPVzQ+HHYBAwiwRzFacj3ucpiL1rvpCwE1gLbkWNbFXglok0SvWrOFuAA0FzQ+I8hIt5Y0PhxuBzxcUWMZzcAO4HjLfai9U5KL/LdDwF9xAv4JHrVlGnIavNCgT68i4hzSoI2FW3TnoM/NvqAPdgDGWA1srKdY7HXg95JuQ44FzgO0TtOwINBrwbHjs1RZ7qARcgWdBD4G9iBOB7kLuSLbHDsS5BGoAP4GhhChHhAx54FDAD7cvbBzwpEq9sNtrMw5xSvI/nGfEufTwOLgXnAD5ZnovROM4d5sw34Xv1KQpRembkXcaoP+Ad4G1gFbNX6IXXAzxf6bJ7J0wRkG68iW/1qYJ3686LWv5OwzwrZVviN2j6oB8AdalsZqJ+v9U8Z2nQhv7AXRIwbpXeaOawl7cRf4cP0csI6HeAgcHXA1mUYvAUR9Zu8HFK8oF6JrG4ec7W+CnQm7LNCtoDfjdyZm3hS+74lUD9Z63cE6p9DkrnrgVN9n2DCFkfvpHNYa9qJH/A2vZzhJRd3GmwXqe09X900rduSl0OMJi8fW+zfqT3sCs9EhfQB36pt37fYvdX0dINtCLlj91O1fDoDz8XRO+kc1pp24gc8mPU6Sj928Uwf/6ATgWFgL+bcoE3bbPXVXal1Yef3LD4BvKz111r63672Noc+dIf0BaNb7RMW++/Y84kBZJVOQ5TeaeYwSD/Z5iuK9oTtxujVGDDuQpKTuPzs+3kGcALy1m/E8OxULff66oa0bAoZI4tPADcgAfSJ5fmzkeAaCOlzDXBioO4S4DbgJWSS/fRG+Dhbyy8t/rRiX/2bGdUtKVF6p5nDIFnnyzVj9AoG/LwMHc/Sst9iv1lL/3a6X8uTQvrN4lMTMAVJVE0Z/hzk2NAT0c8aQ10FCfhu4KOEfs3U8iuD7SYtdxpsDcgv3u6E43lE6Z1mDoNkmS/XjNPL5bWkJ1ZwJQRZsZYCPyFZv8cgkrhNd+iHnyP6mWKxP6qlKfDy5DzgMHI16mcisEx/Nvk0HUm6o3YQG1F6p5nDemacXi4D3tumFzL2ymsS8CqyqnQwdrurIkeNk7G/KMnCYeT+to3xSemDjN7Pmo4WeTKMHB2m+epakBsS768cTSv8FVpuSzlulN5p5rCeyaqXlUbknNSLbB+7gGeQ67IBROiHLG0Xqf0e104pi7X/YeQPplYh11RDyFm0irzoSUqF9Lc0j2vbQUSj9YhOPciZ9gBjr089XkMSsDNSjOlh0zvLHObNAuTo2I3kNlXEP6/Odk3qQi8jF6sTzyOvgHuQe+FDwIdI4mhjAnJt9Jlrp3x0AD8iK/4+4A1k+94D/JayzwrpA74JeBYJ7r+Ql0HLkKPECOacYDISkG+lGM+PTe8sc5g3nYTf9PQb2rjSy8gSHXhpyvbLtf3MqAf/x9yHaHSNg75Memedw3rDpV7jWKudX5ayfROy2m525tGxRTOyG2x01J9J76xzWE+41mscnyJnpbD79CjmAo9wbP5DQlbOR7b1qQ77DOrtYg7rhTz0OkoD8CfwbR6dl9SEcg5LSkpKSkpKSkpKSuqSfwEudslzWIQ9LAAAAABJRU5ErkJggg==\n",
      "text/latex": [
       "$\\displaystyle - p \\left(- d + u\\right)^{2} \\left(p - 1\\right)$"
      ],
      "text/plain": [
       "           2        \n",
       "-p*(-d + u) *(p - 1)"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    ">>> from sympy import *\n",
    ">>> init_printing(use_unicode=False,wrap_line=False,no_global=True)\n",
    ">>> p,u,d=symbols('p u d')\n",
    ">>> m2=p*u**2+(1-p)*d**2\n",
    ">>> m1=p*u+(1-p)*d\n",
    ">>> v=m2-m1**2\n",
    ">>> factor(v)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SNIPPET 15.3 COMPUTING THE IMPLIED PRECISION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid character in identifier (<ipython-input-86-57b8a36c83e9>, line 16)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-86-57b8a36c83e9>\"\u001b[1;36m, line \u001b[1;32m16\u001b[0m\n\u001b[1;33m    p=(-b+(b**2‚Äì4*a*c)**.5)/(2.*a)\u001b[0m\n\u001b[1;37m                ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid character in identifier\n"
     ]
    }
   ],
   "source": [
    "def binHR(sl,pt,freq,tSR):\n",
    "    '''\n",
    "    Given a trading rule characterized by the parameters {sl,pt,freq},\n",
    "    what's the min precision p required to achieve a Sharpe ratio tSR?\n",
    "    1) Inputs\n",
    "    sl: stop loss threshold\n",
    "    pt: profit taking threshold\n",
    "    freq: number of bets per year\n",
    "        tSR: target annual Sharpe ratio\n",
    "    2) Output\n",
    "    p: the min precision rate p required to achieve tSR\n",
    "    '''\n",
    "    a=(freq+tSR**2)*(pt-sl)**2\n",
    "    b=(2*freq*sl-tSR**2*(pt-sl))*(pt-sl)\n",
    "    c=freq*sl**2\n",
    "    p=(-b+(b**2‚Äì4*a*c)**.5)/(2.*a)\n",
    "    return p"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SNIPPET 15.4 COMPUTING THE IMPLIED BETTING FREQUENCY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "def binFreq(sl,pt,p,tSR):\n",
    "    '''\n",
    "    Given a trading rule characterized by the parameters {sl,pt,freq},\n",
    "    what's the number of bets/year needed to achieve a Sharpe ratio\n",
    "    tSR with precision rate p?\n",
    "    Note: Equation with radicals, check for extraneous solution.\n",
    "    1) Inputs\n",
    "    sl: stop loss threshold\n",
    "    pt: profit taking threshold\n",
    "    p: precision rate p\n",
    "    tSR: target annual Sharpe ratio\n",
    "    2) Output\n",
    "    freq: number of bets per year needed\n",
    "    '''\n",
    "    freq=(tSR*(pt-sl))**2*p*(1-p)/((pt-sl)*p+sl)**2 # possible extraneous\n",
    "    if not np.isclose(binSR(sl,pt,freq,p),tSR):return\n",
    "    return freq"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SNIPPET 15.5 CALCULATING THE STRATEGY RISK IN PRACTICE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'binHR' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-90-266be3993235>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     28\u001b[0m     \u001b[1;32mreturn\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     29\u001b[0m \u001b[1;31m#‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 30\u001b[1;33m \u001b[1;32mif\u001b[0m \u001b[0m__name__\u001b[0m\u001b[1;33m==\u001b[0m\u001b[1;34m'__main__'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mmain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-90-266be3993235>\u001b[0m in \u001b[0;36mmain\u001b[1;34m()\u001b[0m\n\u001b[0;32m     24\u001b[0m     \u001b[0mret\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmixGaussians\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmu1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mmu2\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0msigma1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0msigma2\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mprob1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mnObs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     25\u001b[0m     \u001b[1;31m#3) Compute prob failure\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 26\u001b[1;33m     \u001b[0mprobF\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mprobFailure\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mret\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mfreq\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtSR\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     27\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Prob strategy will fail'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mprobF\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     28\u001b[0m     \u001b[1;32mreturn\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-90-266be3993235>\u001b[0m in \u001b[0;36mprobFailure\u001b[1;34m(ret, freq, tSR)\u001b[0m\n\u001b[0;32m     13\u001b[0m     \u001b[0mrPos\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mrNeg\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mret\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mret\u001b[0m\u001b[1;33m>\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mret\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mret\u001b[0m\u001b[1;33m<=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m     \u001b[0mp\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mret\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mret\u001b[0m\u001b[1;33m>\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m/\u001b[0m\u001b[0mfloat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mret\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 15\u001b[1;33m     \u001b[0mthresP\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mbinHR\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrNeg\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mrPos\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mfreq\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtSR\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     16\u001b[0m     \u001b[0mrisk\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnorm\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcdf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mthresP\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mp\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mp\u001b[0m\u001b[1;33m*\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0mp\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m# approximation to bootstrap\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mrisk\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'binHR' is not defined"
     ]
    }
   ],
   "source": [
    "import numpy as np,scipy.stats as ss\n",
    "#‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî\n",
    "def mixGaussians(mu1,mu2,sigma1,sigma2,prob1,nObs):\n",
    "    # Random draws from a mixture of gaussians\n",
    "    ret1=np.random.normal(mu1,sigma1,size=int(nObs*prob1))\n",
    "    ret2=np.random.normal(mu2,sigma2,size=int(nObs)-ret1.shape[0])\n",
    "    ret=np.append(ret1,ret2,axis=0)\n",
    "    np.random.shuffle(ret)\n",
    "    return ret\n",
    "#‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî\n",
    "def probFailure(ret,freq,tSR):\n",
    "    # Derive probability that strategy may fail\n",
    "    rPos,rNeg=ret[ret>0].mean(),ret[ret<=0].mean()\n",
    "    p=ret[ret>0].shape[0]/float(ret.shape[0])\n",
    "    thresP=binHR(rNeg,rPos,freq,tSR)\n",
    "    risk=ss.norm.cdf(thresP,p,p*(1-p)) # approximation to bootstrap\n",
    "    return risk\n",
    "#‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî\n",
    "def main():\n",
    "    #1) Parameters\n",
    "    mu1,mu2,sigma1,sigma2,prob1,nObs=.05,-.1,.05,.1,.75,2600\n",
    "    tSR,freq=2.,260\n",
    "    #2) Generate sample from mixture\n",
    "    ret=mixGaussians(mu1,mu2,sigma1,sigma2,prob1,nObs)\n",
    "    #3) Compute prob failure\n",
    "    probF=probFailure(ret,freq,tSR)\n",
    "    print('Prob strategy will fail',probF)\n",
    "    return\n",
    "#‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî\n",
    "if __name__=='__main__':main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CHAPTER 16 - Machine Learning Asset Allocation\n",
    "\n",
    "#### SNIPPET 16.1 TREE CLUSTERING USING SCIPY FUNCTIONALITY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'int' object has no attribute 'cov'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-91-ae0b965af48a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[0mcov\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mcorr\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcov\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcorr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m \u001b[0mdist\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0mcorr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m/\u001b[0m\u001b[1;36m2.\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m**\u001b[0m\u001b[1;36m.5\u001b[0m \u001b[1;31m# distance matrix\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[0mlink\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlinkage\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdist\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'single'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m# linkage matrix\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'int' object has no attribute 'cov'"
     ]
    }
   ],
   "source": [
    "import scipy.cluster.hierarchy as sch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "cov,corr=x.cov(),x.corr()\n",
    "dist=((1-corr)/2.)**.5 # distance matrix\n",
    "link=sch.linkage(dist,'single') # linkage matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SNIPPET 16.2 QUASI-DIAGONALIZATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getQuasiDiag(link):\n",
    "    # Sort clustered items by distance\n",
    "    link=link.astype(int)\n",
    "    sortIx=pd.Series([link[-1,0],link[-1,1]])\n",
    "    numItems=link[-1,3] # number of original items\n",
    "    while sortIx.max()>=numItems:\n",
    "        sortIx.index=range(0,sortIx.shape[0]*2,2) # make space\n",
    "        df0=sortIx[sortIx>=numItems] # find clusters\n",
    "        i=df0.index;j=df0.values-numItems\n",
    "        sortIx[i]=link[j,0] # item 1\n",
    "        df0=pd.Series(link[j,1],index=i+1)\n",
    "        sortIx=sortIx.append(df0) # item 2\n",
    "        sortIx=sortIx.sort_index() # re-sort\n",
    "        sortIx.index=range(sortIx.shape[0]) # re-index\n",
    "    return sortIx.tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SNIPPET 16.3 RECURSIVE BISECTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getRecBipart(cov,sortIx):\n",
    "    # Compute HRP alloc\n",
    "    w=pd.Series(1,index=sortIx)\n",
    "    cItems=[sortIx] # initialize all items in one cluster\n",
    "    while len(cItems)>0:\n",
    "        cItems=[i[j:k] for i in cItems for j,k in ((0,len(i)/2),\\\n",
    "            (len(i)/2,len(i))) if len(i)>1] # bi-section\n",
    "    for i in xrange(0,len(cItems),2): # parse in pairs\n",
    "        cItems0=cItems[i] # cluster 1\n",
    "        cItems1=cItems[i+1] # cluster 2\n",
    "        cVar0=getClusterVar(cov,cItems0)\n",
    "        cVar1=getClusterVar(cov,cItems1)\n",
    "        alpha=1-cVar0/(cVar0+cVar1)\n",
    "        w[cItems0]*=alpha # weight 1\n",
    "        w[cItems1]*=1-alpha # weight 2\n",
    "    return w"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SNIPPET 16.4 FULL IMPLEMENTATION OF THE HRP ALGORITHM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as mpl\n",
    "import scipy.cluster.hierarchy as sch,random,numpy as np,pandas as pd\n",
    "#‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî\n",
    "def getIVP(cov,**kargs):\n",
    "# Compute the inverse-variance portfolio\n",
    "ivp=1./np.diag(cov)\n",
    "ivp/=ivp.sum()\n",
    "return ivp\n",
    "#‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî\n",
    "def getClusterVar(cov,cItems):\n",
    "# Compute variance per cluster\n",
    "cov_=cov.loc[cItems,cItems] # matrix slice\n",
    "w_=getIVP(cov_).reshape(-1,1)\n",
    "cVar=np.dot(np.dot(w_.T,cov_),w_)[0,0]\n",
    "return cVar\n",
    "#‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî\n",
    "def getQuasiDiag(link):\n",
    "# Sort clustered items by distance\n",
    "link=link.astype(int)\n",
    "sortIx=pd.Series([link[-1,0],link[-1,1]])\n",
    "numItems=link[-1,3] # number of original items\n",
    "while sortIx.max()>=numItems:\n",
    "sortIx.index=range(0,sortIx.shape[0]*2,2) # make space\n",
    "df0=sortIx[sortIx>=numItems] # find clusters\n",
    "i=df0.index;j=df0.values-numItems\n",
    "sortIx[i]=link[j,0] # item 1\n",
    "df0=pd.Series(link[j,1],index=i+1)\n",
    "sortIx=sortIx.append(df0) # item 2\n",
    "sortIx=sortIx.sort_index() # re-sort\n",
    "sortIx.index=range(sortIx.shape[0]) # re-index\n",
    "return sortIx.tolist()\n",
    "#‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî\n",
    "def getRecBipart(cov,sortIx):\n",
    "# Compute HRP alloc\n",
    "w=pd.Series(1,index=sortIx)\n",
    "cItems=[sortIx] # initialize all items in one cluster\n",
    "while len(cItems)>0:\n",
    "cItems=[i[j:k] for i in cItems for j,k in ((0,len(i)/2), \\\n",
    "(len(i)/2,len(i))) if len(i)>1] # bi-section\n",
    "for i in xrange(0,len(cItems),2): # parse in pairs\n",
    "cItems0=cItems[i] # cluster 1\n",
    "cItems1=cItems[i+1] # cluster 2\n",
    "cVar0=getClusterVar(cov,cItems0)\n",
    "cVar1=getClusterVar(cov,cItems1)\n",
    "alpha=1-cVar0/(cVar0+cVar1)\n",
    "w[cItems0]*=alpha # weight 1\n",
    "w[cItems1]*=1-alpha # weight 2\n",
    "return w\n",
    "#‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî\n",
    "def correlDist(corr):\n",
    "# A distance matrix based on correlation, where 0<=d[i,j]<=1\n",
    "# This is a proper distance metric\n",
    "dist=((1-corr)/2.)**.5 # distance matrix\n",
    "return dist\n",
    "#‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî\n",
    "def plotCorrMatrix(path,corr,labels=None):\n",
    "# Heatmap of the correlation matrix\n",
    "if labels is None:labels=[]\n",
    "mpl.pcolor(corr)\n",
    "mpl.colorbar()\n",
    "mpl.yticks(np.arange(.5,corr.shape[0]+.5),labels)\n",
    "mpl.xticks(np.arange(.5,corr.shape[0]+.5),labels)\n",
    "mpl.savefig(path)\n",
    "mpl.clf();mpl.close() # reset pylab\n",
    "return\n",
    "#‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî\n",
    "def generateData(nObs,size0,size1,sigma1):\n",
    "# Time series of correlated variables\n",
    "#1) generating some uncorrelated data\n",
    "np.random.seed(seed=12345);random.seed(12345)\n",
    "x=np.random.normal(0,1,size=(nObs,size0)) # each row is a variable\n",
    "#2) creating correlation between the variables\n",
    "cols=[random.randint(0,size0‚Äì1) for i in xrange(size1)]\n",
    "y=x[:,cols]+np.random.normal(0,sigma1,size=(nObs,len(cols)))\n",
    "x=np.append(x,y,axis=1)\n",
    "x=pd.DataFrame(x,columns=range(1,x.shape[1]+1))\n",
    "return x,cols\n",
    "#‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî\n",
    "def main():\n",
    "#1) Generate correlated data\n",
    "nObs,size0,size1,sigma1=10000,5,5,.25\n",
    "x,cols=generateData(nObs,size0,size1,sigma1)\n",
    "print [(j+1,size0+i) for i,j in enumerate(cols,1)]\n",
    "cov,corr=x.cov(),x.corr()\n",
    "#2) compute and plot correl matrix\n",
    "plotCorrMatrix('HRP3_corr0.png',corr,labels=corr.columns)\n",
    "#3) cluster\n",
    "dist=correlDist(corr)\n",
    "link=sch.linkage(dist,'single')\n",
    "sortIx=getQuasiDiag(link)\n",
    "sortIx=corr.index[sortIx].tolist() # recover labels\n",
    "df0=corr.loc[sortIx,sortIx] # reorder\n",
    "plotCorrMatrix('HRP3_corr1.png',df0,labels=df0.columns)\n",
    "#4) Capital allocation\n",
    "hrp=getRecBipart(cov,sortIx)\n",
    "print hrp\n",
    "return\n",
    "#‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî\n",
    "if __name__=='__main__':main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SNIPPET 16.5 MONTE CARLO EXPERIMENT ON HRP OUT-OF-SAMPLE PERFORMANCE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(...)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CHAPTER 17 - Structural Breaks\n",
    "#### SNIPPET 17.1 SADF‚ÄôS INNER LOOP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_bsadf(logP,minSL,constant,lags):\n",
    "    y,x=getYX(logP,constant=constant,lags=lags)\n",
    "    startPoints,bsadf,allADF=range(0,y.shape[0]+lags-minSL+1),None,[]\n",
    "    for start in startPoints:\n",
    "        y_,x_=y[start:],x[start:]\n",
    "        bMean_,bStd_=getBetas(y_,x_)\n",
    "        bMean_,bStd_=bMean_[0,0],bStd_[0,0]**.5\n",
    "        allADF.append(bMean_/bStd_)\n",
    "        if allADF[-1]>bsadf:bsadf=allADF[-1]\n",
    "    out={'Time':logP.index[-1],'gsadf':bsadf}\n",
    "    return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SNIPPET 17.2 PREPARING THE DATASETS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getYX(series,constant,lags):\n",
    "    series_=series.diff().dropna()\n",
    "    x=lagDF(series_,lags).dropna()\n",
    "    x.iloc[:,0]=series.values[-x.shape[0]-1:-1,0] # lagged level\n",
    "    y=series_.iloc[-x.shape[0]:].values\n",
    "    if constant!='nc':\n",
    "        x=np.append(x,np.ones((x.shape[0],1)),axis=1)\n",
    "        if constant[:2]=='ct':\n",
    "            trend=np.arange(x.shape[0]).reshape(-1,1)\n",
    "            x=np.append(x,trend,axis=1)\n",
    "        if constant=='ctt':\n",
    "            x=np.append(x,trend**2,axis=1)\n",
    "    return y,x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SNIPPET 17.3 APPLY LAGS TO DATAFRAME"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lagDF(df0,lags):\n",
    "    df1=pd.DataFrame()\n",
    "    if isinstance(lags,int):lags=range(lags+1)\n",
    "    else:lags=[int(lag) for lag in lags]\n",
    "    for lag in lags:\n",
    "        df_=df0.shift(lag).copy(deep=True)\n",
    "        df_.columns=[str(i)+'_'+str(lag) for i in df_.columns]\n",
    "        df1=df1.join(df_,how='outer')\n",
    "    return df1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SNIPPET 17.4 FITTING THE ADF SPECIFICATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getBetas(y,x):\n",
    "    xy=np.dot(x.T,y)\n",
    "    xx=np.dot(x.T,x)\n",
    "    xxinv=np.linalg.inv(xx)\n",
    "    bMean=np.dot(xxinv,xy)\n",
    "    err=y-np.dot(x,bMean)\n",
    "    bVar=np.dot(err.T,err)/(x.shape[0]-x.shape[1])*xxinv\n",
    "    return bMean,bVar"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CHAPTER 18 - Entropy Features\n",
    "#### SNIPPET 18.1 PLUG-IN ENTROPY ESTIMATOR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time,numpy as np\n",
    "#‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî\n",
    "def plugIn(msg,w):\n",
    "    # Compute plug-in (ML) entropy rate\n",
    "    pmf=pmf1(msg,w)\n",
    "    out=-sum([pmf[i]*np.log2(pmf[i]) for i in pmf])/w\n",
    "    return out,pmf\n",
    "#‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî\n",
    "def pmf1(msg,w):\n",
    "    # Compute the prob mass function for a one-dim discrete rv\n",
    "    # len(msg)-w occurrences\n",
    "    lib={}\n",
    "    if not isinstance(msg,str):msg=''.join(map(str,msg))\n",
    "    for i in xrange(w,len(msg)):\n",
    "        msg_=msg[i-w:i]\n",
    "        if msg_ not in lib:lib[msg_]=[i-w]\n",
    "        else:lib[msg_]=lib[msg_]+[i-w]\n",
    "    pmf=float(len(msg)-w)\n",
    "    pmf={i:len(lib[i])/pmf for i in lib}\n",
    "    return pmf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SNIPPET 18.2 A LIBRARY BUILT USING THE LZ ALGORITHM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lempelZiv_lib(msg):\n",
    "    i,lib=1,[msg[0]]\n",
    "    while i<len(msg):\n",
    "        for j in xrange(i,len(msg)):\n",
    "            msg_=msg[i:j+1]\n",
    "            if msg_ not in lib:\n",
    "                lib.append(msg_)\n",
    "                break\n",
    "    i=j+1\n",
    "    return lib"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SNIPPET 18.3 FUNCTION THAT COMPUTES THE LENGTH OF THE LONGEST MATCH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "def matchLength(msg,i,n):\n",
    "    # Maximum matched length+1, with overlap.\n",
    "    # i>=n & len(msg)>=i+n\n",
    "    subS=''\n",
    "    for l in xrange(n):\n",
    "        msg1=msg[i:i+l+1]\n",
    "        for j in xrange(i-n,i):\n",
    "            msg0=msg[j:j+l+1]\n",
    "            if msg1==msg0:\n",
    "                subS=msg1\n",
    "                break # search for higher l.\n",
    "    return len(subS)+1,subS # matched length + 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SNIPPET 18.4 IMPLEMENTATION OF ALGORITHMS DISCUSSED IN GAO ET AL. [2008]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'float' object cannot be interpreted as an integer",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-104-cb0d1089d345>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     29\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0m__name__\u001b[0m\u001b[1;33m==\u001b[0m\u001b[1;34m'__main__'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     30\u001b[0m     \u001b[0mmsg\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'101010'\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 31\u001b[1;33m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkonto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[1;33m*\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     32\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkonto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0mmsg\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-104-cb0d1089d345>\u001b[0m in \u001b[0;36mkonto\u001b[1;34m(msg, window)\u001b[0m\n\u001b[0;32m     10\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mmsg\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m''\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmap\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mmsg\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mwindow\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 12\u001b[1;33m         \u001b[0mpoints\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m/\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     13\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m         \u001b[0mwindow\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mwindow\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m/\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: 'float' object cannot be interpreted as an integer"
     ]
    }
   ],
   "source": [
    "def konto(msg,window=None):\n",
    "    '''\n",
    "    * Kontoyiannis‚Äô LZ entropy estimate, 2013 version (centered window).\n",
    "    * Inverse of the avg length of the shortest non-redundant substring.\n",
    "    * If non-redundant substrings are short, the text is highly entropic.\n",
    "    * window==None for expanding window, in which case len(msg)%2==0\n",
    "    * If the end of msg is more relevant, try konto(msg[::-1])\n",
    "    '''\n",
    "    out={'num':0,'sum':0,'subS':[]}\n",
    "    if not isinstance(msg,str):msg=''.join(map(str,msg))\n",
    "    if window is None:\n",
    "        points=range(1,len(msg)/2+1)\n",
    "    else:\n",
    "        window=min(window,len(msg)/2)\n",
    "        points=xrange(window,len(msg)-window+1)\n",
    "    for i in points:\n",
    "        if window is None:\n",
    "            l,msg_=matchLength(msg,i,i)\n",
    "            out['sum']+=np.log2(i+1)/l # to avoid Doeblin condition\n",
    "        else:\n",
    "            l,msg_=matchLength(msg,i,window)\n",
    "            out['sum']+=np.log2(window+1)/l # to avoid Doeblin condition\n",
    "        out['subS'].append(msg_)\n",
    "        out['num']+=1\n",
    "    out['h']=out['sum']/out['num']\n",
    "    out['r']=1-out['h']/np.log2(len(msg)) # redundancy, 0<=r<=1\n",
    "    return out\n",
    "#‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî\n",
    "if __name__=='__main__':\n",
    "    msg='101010'\n",
    "    print(konto(msg*2))\n",
    "    print(konto(msg+msg[::-1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CHAPTER 19 - Microstructural Features\n",
    "#### SNIPPET 19.1 IMPLEMENTATION OF THE CORWIN-SCHULTZ ALGORITHM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid character in identifier (<ipython-input-105-6c32cf2e01c3>, line 17)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-105-6c32cf2e01c3>\"\u001b[1;36m, line \u001b[1;32m17\u001b[0m\n\u001b[1;33m    den=3‚Äì2*2**.5\u001b[0m\n\u001b[1;37m          ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid character in identifier\n"
     ]
    }
   ],
   "source": [
    "def getBeta(series,sl):\n",
    "    hl=series[['High','Low']].values\n",
    "    hl=np.log(hl[:,0]/hl[:,1])**2\n",
    "    hl=pd.Series(hl,index=series.index)\n",
    "    beta=pd.stats.moments.rolling_sum(hl,window=2)\n",
    "    beta=pd.stats.moments.rolling_mean(beta,window=sl)\n",
    "    return beta.dropna()\n",
    "#‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî-\n",
    "def getGamma(series):\n",
    "    h2=pd.stats.moments.rolling_max(series['High'],window=2)\n",
    "    l2=pd.stats.moments.rolling_min(series['Low'],window=2)\n",
    "    gamma=np.log(h2.values/l2.values)**2\n",
    "    gamma=pd.Series(gamma,index=h2.index)\n",
    "    return gamma.dropna()\n",
    "#‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî-\n",
    "def getAlpha(beta,gamma):\n",
    "    den=3‚Äì2*2**.5\n",
    "    alpha=(2**.5‚Äì1)*(beta**.5)/den\n",
    "    alpha-=(gamma/den)**.5\n",
    "    alpha[alpha<0]=0 # set negative alphas to 0 (see p.727 of paper)\n",
    "    return alpha.dropna()\n",
    "#‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî-\n",
    "def corwinSchultz(series,sl=1):\n",
    "    # Note: S<0 iif alpha<0\n",
    "    beta=getBeta(series,sl)\n",
    "    gamma=getGamma(series)\n",
    "    alpha=getAlpha(beta,gamma)\n",
    "    spread=2*(np.exp(alpha)-1)/(1+np.exp(alpha))\n",
    "    startTime=pd.Series(series.index[0:spread.shape[0]],index=spread.index)\n",
    "    spread=pd.concat([spread,startTime],axis=1)\n",
    "    spread.columns=['Spread','Start_Time'] # 1st loc used to compute beta\n",
    "    return spread"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SNIPPET 19.2 ESTIMATING VOLATILITY FOR HIGH-LOW PRICES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid character in identifier (<ipython-input-106-fbbf1b8029c7>, line 3)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-106-fbbf1b8029c7>\"\u001b[1;36m, line \u001b[1;32m3\u001b[0m\n\u001b[1;33m    den=3‚Äì2*2**.5\u001b[0m\n\u001b[1;37m          ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid character in identifier\n"
     ]
    }
   ],
   "source": [
    "def getSigma(beta,gamma):\n",
    "    k2=(8/np.pi)**.5\n",
    "    den=3‚Äì2*2**.5\n",
    "    sigma=(2**-.5‚Äì1)*beta**.5/(k2*den)\n",
    "    sigma+=(gamma/(k2**2*den))**.5\n",
    "    sigma[sigma<0]=0\n",
    "    return sigma"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CHAPTER 20 - Multiprocessing and Vectorization\n",
    "#### SNIPPET 20.1 UN-VECTORIZED CARTESIAN PRODUCT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'a': '1', 'b': '+', 'c': '!'}\n",
      "{'a': '1', 'b': '+', 'c': '@'}\n",
      "{'a': '1', 'b': '*', 'c': '!'}\n",
      "{'a': '1', 'b': '*', 'c': '@'}\n",
      "{'a': '2', 'b': '+', 'c': '!'}\n",
      "{'a': '2', 'b': '+', 'c': '@'}\n",
      "{'a': '2', 'b': '*', 'c': '!'}\n",
      "{'a': '2', 'b': '*', 'c': '@'}\n"
     ]
    }
   ],
   "source": [
    "# Cartesian product of dictionary of lists\n",
    "dict0={'a':['1','2'],'b':['+','*'],'c':['!','@']}\n",
    "for a in dict0['a']:\n",
    "    for b in dict0['b']:\n",
    "        for c in dict0['c']:\n",
    "            print({'a':a,'b':b,'c':c})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SNIPPET 20.2 VECTORIZED CARTESIAN PRODUCT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'izip' from 'itertools' (unknown location)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-112-d7f6b92a3e78>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# Cartesian product of dictionary of lists\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mitertools\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mizip\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mproduct\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0mdict0\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m{\u001b[0m\u001b[1;34m'a'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'1'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'2'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'b'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'+'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'*'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'c'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'!'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'@'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mjobs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mizip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdict0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mproduct\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mdict0\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mjobs\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mImportError\u001b[0m: cannot import name 'izip' from 'itertools' (unknown location)"
     ]
    }
   ],
   "source": [
    "# Cartesian product of dictionary of lists\n",
    "from itertools import izip,product\n",
    "dict0={'a':['1','2'],'b':['+','*'],'c':['!','@']}\n",
    "jobs=(dict(izip(dict0,i)) for i in product(*dict0.values()))\n",
    "for i in jobs: print(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SNIPPET 20.3 SINGLE-THREAD IMPLEMENTATION OF A ONE-TOUCH DOUBLE BARRIER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "49.66141710000011\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "#‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî\n",
    "def main0():\n",
    "    # Path dependency: Sequential implementation\n",
    "    r=np.random.normal(0,.01,size=(1000,10000))\n",
    "    t=barrierTouch(r)\n",
    "    return\n",
    "#‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî\n",
    "def barrierTouch(r,width=.5):\n",
    "    # find the index of the earliest barrier touch\n",
    "    t,p={},np.log((1+r).cumprod(axis=0))\n",
    "    for j in range(r.shape[1]): # go through columns\n",
    "        for i in range(r.shape[0]): # go through rows\n",
    "            if p[i,j]>=width or p[i,j]<=-width:\n",
    "                t[j]=i\n",
    "                continue\n",
    "    return t\n",
    "#‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî\n",
    "if __name__=='__main__':\n",
    "    import timeit\n",
    "    print(min(timeit.Timer('main0()',setup='from __main__ import main0').repeat(5,10)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SNIPPET 20.4 MULTIPROCESSING IMPLEMENTATION OF A ONE-TOUCH DOUBLE BARRIER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import multiprocessing as mp\n",
    "#‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî\n",
    "def main1():\n",
    "    # Path dependency: Multi-threaded implementation\n",
    "    r,numThreads=np.random.normal(0,.01,size=(1000,10000)),24\n",
    "    parts=np.linspace(0,r.shape[0],min(numThreads,r.shape[0])+1)\n",
    "    parts,jobs=np.ceil(parts).astype(int),[]\n",
    "    for i in range(1,len(parts)):\n",
    "        jobs.append(r[:,parts[i-1]:parts[i]]) # parallel jobs\n",
    "    pool,out=mp.Pool(processes=numThreads),[]\n",
    "    outputs=pool.imap_unordered(barrierTouch,jobs)\n",
    "    for out_ in outputs:out.append(out_) # asynchronous response\n",
    "    pool.close();pool.join()\n",
    "    return\n",
    "#‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî\n",
    "if __name__=='__main__':\n",
    "    import timeit\n",
    "    print(min(timeit.Timer('main1()',setup='from __main__ import main1').repeat(5,10)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SNIPPET 20.5 THE linParts FUNCTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "#‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî\n",
    "def linParts(numAtoms,numThreads):\n",
    "    # partition of atoms with a single loop\n",
    "    parts=np.linspace(0,numAtoms,min(numThreads,numAtoms)+1)\n",
    "    parts=np.ceil(parts).astype(int)\n",
    "    return parts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SNIPPET 20.6 THE nestedParts FUNCTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nestedParts(numAtoms,numThreads,upperTriang=False):\n",
    "    # partition of atoms with an inner loop\n",
    "    parts,numThreads_=[0],min(numThreads,numAtoms)\n",
    "    for num in xrange(numThreads_):\n",
    "        part=1 + 4*(parts[-1]**2+parts[-1]+numAtoms*(numAtoms+1.)/numThreads_)\n",
    "        part=(-1+part**.5)/2.\n",
    "        parts.append(part)\n",
    "    parts=np.round(parts).astype(int)\n",
    "    if upperTriang: # the first rows are the heaviest\n",
    "        parts=np.cumsum(np.diff(parts)[::-1])\n",
    "        parts=np.append(np.array([0]),parts)\n",
    "    return parts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SNIPPET 20.7 THE mpPandasObj, USED AT VARIOUS POINTS IN THE BOOK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mpPandasObj(func,pdObj,numThreads=24,mpBatches=1,linMols=True,**kargs):\n",
    "    '''\n",
    "    Parallelize jobs, return a DataFrame or Series\n",
    "    + func: function to be parallelized. Returns a DataFrame\n",
    "    + pdObj[0]: Name of argument used to pass the molecule\n",
    "    + pdObj[1]: List of atoms that will be grouped into molecules\n",
    "    + kargs: any other argument needed by func\n",
    "    Example: df1=mpPandasObj(func,(‚Äômolecule‚Äô,df0.index),24,**kargs)\n",
    "    '''\n",
    "    import pandas as pd\n",
    "    if linMols:parts=linParts(len(argList[1]),numThreads*mpBatches)\n",
    "    else:parts=nestedParts(len(argList[1]),numThreads*mpBatches)\n",
    "    jobs=[] for i in xrange(1,len(parts)):\n",
    "        job={pdObj[0]:pdObj[1][parts[i-1]:parts[i]],'func':func}\n",
    "        job.update(kargs)\n",
    "        jobs.append(job)\n",
    "    if numThreads==1:out=processJobs_(jobs)\n",
    "    else:out=processJobs(jobs,numThreads=numThreads)\n",
    "    if isinstance(out[0],pd.DataFrame):df0=pd.DataFrame()\n",
    "    elif isinstance(out[0],pd.Series):df0=pd.Series()\n",
    "    else:return out\n",
    "    for i in out:df0=df0.append(i)\n",
    "    df0=df0.sort_index()\n",
    "    return df0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SNIPPET 20.8 SINGLE-THREAD EXECUTION, FOR DEBUGGING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def processJobs_(jobs):\n",
    "    # Run jobs sequentially, for debugging\n",
    "    out=[]\n",
    "    for job in jobs:\n",
    "        out_=expandCall(job)\n",
    "        out.append(out_)\n",
    "    return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SNIPPET 20.9 EXAMPLE OF ASYNCHRONOUS CALL TO PYTHON‚ÄôS MULTIPROCESSING LIBRARY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import multiprocessing as mp\n",
    "#‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî\n",
    "def reportProgress(jobNum,numJobs,time0,task):\n",
    "    # Report progress as asynch jobs are completed\n",
    "    msg=[float(jobNum)/numJobs,(time.time()-time0)/60.]\n",
    "    msg.append(msg[1]*(1/msg[0]-1))\n",
    "    timeStamp=str(dt.datetime.fromtimestamp(time.time()))\n",
    "    msg=timeStamp+' '+str(round(msg[0]*100,2))+'% '+task+' done after '+ \\\n",
    "    str(round(msg[1],2))+' minutes. Remaining '+str(round(msg[2],2))+' minutes.'\n",
    "    if jobNum<numJobs:sys.stderr.write(msg+'\\r')\n",
    "    else:sys.stderr.write(msg+'\\n')\n",
    "    return\n",
    "#‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî\n",
    "def processJobs(jobs,task=None,numThreads=24):\n",
    "    # Run in parallel.\n",
    "    # jobs must contain a ‚Äôfunc‚Äô callback, for expandCall\n",
    "    if task is None:task=jobs[0]['func'].__name__\n",
    "    pool=mp.Pool(processes=numThreads)\n",
    "    outputs,out,time0=pool.imap_unordered(expandCall,jobs),[],time.time()\n",
    "    # Process asynchronous output, report progress\n",
    "    for i,out_ in enumerate(outputs,1):\n",
    "        out.append(out_)\n",
    "        reportProgress(i,len(jobs),time0,task)\n",
    "    pool.close();pool.join() # this is needed to prevent memory leaks\n",
    "    return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SNIPPET 20.10 PASSING THE JOB (MOLECULE) TO THE CALLBACK FUNCTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def expandCall(kargs):\n",
    "    # Expand the arguments of a callback function, kargs[‚Äôfunc‚Äô]\n",
    "    func=kargs['func']\n",
    "    del kargs['func']\n",
    "    out=func(**kargs)\n",
    "    return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SNIPPET 20.11 PLACE THIS CODE AT THE BEGINNING OF YOUR ENGINE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _pickle_method(method):\n",
    "    func_name=method.im_func.__name__\n",
    "    obj=method.im_self\n",
    "    cls=method.im_class\n",
    "    return _unpickle_method,(func_name,obj,cls)\n",
    "#‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî\n",
    "def _unpickle_method(func_name,obj,cls):\n",
    "    for cls in cls.mro():\n",
    "        try:func=cls.__dict__[func_name]\n",
    "        except KeyError:pass\n",
    "        else:break\n",
    "    return func.__get__(obj,cls)\n",
    "#‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî\n",
    "import copy_reg,types,multiprocessing as mp\n",
    "copy_reg.pickle(types.MethodType,_pickle_method,_unpickle_method)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SNIPPET 20.12 ENHANCING processJobs TO PERFORM ON-THE-FLY OUTPUT REDUCTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def processJobsRedux(jobs,task=None,numThreads=24,redux=None,reduxArgs={},\n",
    "                    reduxInPlace=False):\n",
    "    '''\n",
    "    Run in parallel\n",
    "    jobs must contain a ‚Äôfunc‚Äô callback, for expandCall\n",
    "    redux prevents wasting memory by reducing output on the fly\n",
    "    '''\n",
    "    if task is None:task=jobs[0]['func'].__name__\n",
    "    pool=mp.Pool(processes=numThreads)\n",
    "    imap,out,time0=pool.imap_unordered(expandCall,jobs),None,time.time()\n",
    "    # Process asynchronous output, report progress\n",
    "    for i,out_ in enumerate(imap,1):\n",
    "        if out is None:\n",
    "            if redux is None:out,redux,reduxInPlace=[out_],list.append,True\n",
    "            else:out=copy.deepcopy(out_)\n",
    "        else:\n",
    "            if reduxInPlace:redux(out,out_,**reduxArgs)\n",
    "            else:out=redux(out,out_,**reduxArgs)\n",
    "        reportProgress(i,len(jobs),time0,task)\n",
    "    pool.close();pool.join() # this is needed to prevent memory leaks\n",
    "    if isinstance(out,(pd.Series,pd.DataFrame)):out=out.sort_index()\n",
    "    return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SNIPPET 20.13 ENHANCING mpPandasObj TO PERFORM ON-THE-FLY OUTPUT REDUCTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mpJobList(func,argList,numThreads=24,mpBatches=1,linMols=True,redux=None,\n",
    "            reduxArgs={},reduxInPlace=False,**kargs):\n",
    "    if linMols:parts=linParts(len(argList[1]),numThreads*mpBatches)\n",
    "    else:parts=nestedParts(len(argList[1]),numThreads*mpBatches)\n",
    "    jobs=[]\n",
    "    for i in xrange(1,len(parts)):\n",
    "        job={argList[0]:argList[1][parts[i-1]:parts[i]],'func':func}\n",
    "        job.update(kargs)\n",
    "        jobs.append(job)\n",
    "    out=processJobsRedux(jobs,redux=redux,reduxArgs=reduxArgs,\n",
    "                    reduxInPlace=reduxInPlace,numThreads=numThreads)\n",
    "    return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SNIPPET 20.14 PRINCIPAL COMPONENTS FOR A SUBSET OF THE COLUMNS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pcs=mpJobList(getPCs,('molecules',fileNames),numThreads=24,mpBatches=1,\n",
    "    path=path,eVec=eVec,redux=pd.DataFrame.add)\n",
    "#‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî\n",
    "def getPCs(path,molecules,eVec):\n",
    "    # get principal components by loading one file at a time\n",
    "    pcs=None\n",
    "    for i in molecules:\n",
    "        df0=pd.read_csv(path+i,index_col=0,parse_dates=True)\n",
    "        if pcs is None:pcs=np.dot(df0.values,eVec.loc[df0.columns].values)\n",
    "        else:pcs+=np.dot(df0.values,eVec.loc[df0.columns].values)\n",
    "    pcs=pd.DataFrame(pcs,index=df0.index,columns=eVec.columns)\n",
    "    return pcs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CHAPTER 21 - Brute Force and Quantum Computers\n",
    "#### SNIPPET 21.1 PARTITIONS OF k OBJECTS INTO n SLOTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import combinations_with_replacement\n",
    "#‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî\n",
    "def pigeonHole(k,n):\n",
    "    # Pigeonhole problem (organize k objects in n slots)\n",
    "    for j in combinations_with_replacement(xrange(n),k):\n",
    "        r=[0]*n\n",
    "        for i in j:\n",
    "            r[i]+=1\n",
    "        yield r"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SNIPPET 21.2 SET ùõÄ OF ALL VECTORS ASSOCIATED WITH ALL PARTITIONS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from itertools import product\n",
    "#‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî\n",
    "def getAllWeights(k,n):\n",
    "    #1) Generate partitions\n",
    "    parts,w=pigeonHole(k,n),None\n",
    "    #2) Go through partitions\n",
    "    for part_ in parts:\n",
    "        w_=np.array(part_)/float(k) # abs(weight) vector\n",
    "        for prod_ in product([-1,1],repeat=n): # add sign\n",
    "            w_signed_=(w_*prod_).reshape(-1,1)\n",
    "            if w is None:w=w_signed_.copy()\n",
    "            else:w=np.append(w,w_signed_,axis=1)\n",
    "    return w"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SNIPPET 21.3 EVALUATING ALL TRAJECTORIES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from itertools import product\n",
    "#‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî\n",
    "def evalTCosts(w,params):\n",
    "    # Compute t-costs of a particular trajectory\n",
    "    tcost=np.zeros(w.shape[1])\n",
    "    w_=np.zeros(shape=w.shape[0])\n",
    "    for i in range(tcost.shape[0]):\n",
    "        c_=params[i]['c']\n",
    "        tcost[i]=(c_*abs(w[:,i]-w_)**.5).sum()\n",
    "        w_=w[:,i].copy()\n",
    "    return tcost\n",
    "#‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî\n",
    "def evalSR(params,w,tcost):\n",
    "    # Evaluate SR over multiple horizons\n",
    "    mean,cov=0,0\n",
    "    for h in range(w.shape[1]):\n",
    "        params_=params[h]\n",
    "        mean+=np.dot(w[:,h].T,params_['mean'])[0]-tcost[h]\n",
    "        cov+=np.dot(w[:,h].T,np.dot(params_['cov'],w[:,h]))\n",
    "    sr=mean/cov**.5\n",
    "    return sr\n",
    "#‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî\n",
    "def dynOptPort(params,k=None):\n",
    "    # Dynamic optimal portfolio\n",
    "    #1) Generate partitions\n",
    "    if k is None:k=params[0]['mean'].shape[0]\n",
    "    n=params[0]['mean'].shape[0]\n",
    "    w_all,sr=getAllWeights(k,n),None\n",
    "    #2) Generate trajectories as cartesian products\n",
    "    for prod_ in product(w_all.T,repeat=len(params)):\n",
    "        w_=np.array(prod_).T # concatenate product into a trajectory\n",
    "        tcost_=evalTCosts(w_,params)\n",
    "        sr_=evalSR(params,w_,tcost_) # evaluate trajectory\n",
    "        if sr is None or sr<sr_: # store trajectory if better\n",
    "            sr,w=sr_,w_.copy()\n",
    "    return w"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SNIPPET 21.4 PRODUCE A RANDOM MATRIX OF A GIVEN RANK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "#‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî\n",
    "def rndMatWithRank(nSamples,nCols,rank,sigma=0,homNoise=True):\n",
    "    # Produce random matrix X with given rank\n",
    "    rng=np.random.RandomState()\n",
    "    U,_,_=np.linalg.svd(rng.randn(nCols,nCols))\n",
    "    x=np.dot(rng.randn(nSamples,rank),U[:,:rank].T)\n",
    "    if homNoise:\n",
    "        x+=sigma*rng.randn(nSamples,nCols) # Adding homoscedastic noise\n",
    "    else:\n",
    "        sigmas=sigma*(rng.rand(nCols)+.5) # Adding heteroscedastic noise\n",
    "        x+=rng.randn(nSamples,nCols)*sigmas\n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SNIPPET 21.5 GENERATE THE PROBLEM‚ÄôS PARAMETERS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "#‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî\n",
    "def genMean(size):\n",
    "    # Generate a random vector of means\n",
    "    rMean=np.random.normal(size=(size,1))\n",
    "    return rMean\n",
    "#‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî\n",
    "    #1) Parameters\n",
    "    size,horizon=3,2\n",
    "    params=[]\n",
    "    for h in range(horizon):\n",
    "        x=rndMatWithRank(1000,3,3,0.)\n",
    "        mean_,cov_=genMean(size),np.cov(x,rowvar=False)\n",
    "        c_=np.random.uniform(size=cov_.shape[0])*np.diag(cov_)**.5\n",
    "        params.append({'mean':mean_,'cov':cov_,'c':c_})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SNIPPET 21.6 COMPUTE AND EVALUATE THE STATIC SOLUTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "#‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî\n",
    "def statOptPortf(cov,a):\n",
    "    # Static optimal porftolio\n",
    "    # Solution to the \"unconstrained\" portfolio optimization problem\n",
    "    cov_inv=np.linalg.inv(cov)\n",
    "    w=np.dot(cov_inv,a)\n",
    "    w/=np.dot(np.dot(a.T,cov_inv),a) # np.dot(w.T,a)==1\n",
    "    w/=abs(w).sum() # re-scale for full investment\n",
    "    return w\n",
    "#‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî\n",
    "#2) Static optimal portfolios\n",
    "w_stat=None\n",
    "for params_ in params:\n",
    "    w_=statOptPortf(cov=params_['cov'],a=params_['mean'])\n",
    "    if w_stat is None:w_stat=w_.copy()\n",
    "    else:w_stat=np.append(w_stat,w_,axis=1)\n",
    "tcost_stat=evalTCosts(w_stat,params)\n",
    "sr_stat=evalSR(params,w_stat,tcost_stat)\n",
    "print 'static SR:',sr_stat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SNIPPET 21.7 COMPUTE AND EVALUATE THE DYNAMIC SOLUTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "#‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî\n",
    "#3) Dynamic optimal portfolios\n",
    "w_dyn=dynOptPort(params)\n",
    "tcost_dyn=evalTCosts(w_dyn,params)\n",
    "sr_dyn=evalSR(params,w_dyn,tcost_dyn)\n",
    "print 'dynamic SR:',sr_dyn"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
